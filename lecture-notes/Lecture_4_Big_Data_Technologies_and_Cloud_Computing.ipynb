{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<body>\n",
        "  <center>\n",
        "    <img align=\"center\" src=\"https://duk.ac.in/wp-content/uploads/2021/03/logo_transparent_bg_caption.png\" width=\"200px\" padding=\"10px\"   style=\" width:300px; padding: 10px;  \" />\n",
        "  <div class=\"university-details\">\n",
        "    <h2> </h2>\n",
        "    <center><h2>Kerala University of Digital Sciences, Innovation and Technology</h2></center> <h3>(Digital University Kerala, DUK)</h3>\n",
        "    <h4><i><b>School of Digital Sciences</b></i><br></h4>\n",
        "  </div>\n",
        "  <h1>  </h1>\n",
        "  </center>\n",
        "  <h5><b>Date:</b> 28/07/2025 <br>\n",
        "<b>Venue:</b> Room Number 45<br>\n",
        "<b>Topic:</b> Apache Spark Intro<br>\n",
        "<b>Instructor:</b> Dr. Aswin VS</h5>\n",
        "</body>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LRJywlS0G4wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apache Spark\n",
        " A Fast and Powerful Cluster Computing Framework"
      ],
      "metadata": {
        "id": "GE4r8nspcFkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Introduction to Apache Spark"
      ],
      "metadata": {
        "id": "lU_M45Z3cIeY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bac1c04"
      },
      "source": [
        "\n",
        "Apache Spark is an open-source, distributed computing framework designed for fast and scalable data processing, making it a key tool in big data analytics. Unlike traditional systems such as Hadoop MapReduce, Spark performs in-memory computation, which significantly speeds up iterative tasks like machine learning, graph processing, and interactive data analysis. It supports multiple programming languages including Python, Java, Scala, and R, and provides high-level APIs as well as specialized libraries such as Spark SQL for structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming for real-time analytics. At its foundation lies **Spark Core**, the general execution engine that provides basic functionalities like task scheduling, memory management, fault recovery, and interaction with storage systems. **Spark Context** acts as the entry point to a Spark application, serving as the connection between the driver program and the Spark cluster, and is responsible for coordinating jobs, distributing data, and managing resources. Together, Spark Core and Spark Context ensure that Spark can efficiently manage large-scale data processing while maintaining flexibility and ease of use.\n",
        "\n",
        "\n",
        "* **Why Spark?**\n",
        "\n",
        "  The need for Spark arose from the limitations of earlier distributed processing systems, particularly Hadoop MapReduce. While MapReduce was a groundbreaking technology for processing large datasets across clusters, it had several drawbacks:\n",
        "\n",
        "\n",
        "  * **Disk I/O Overhead (Too Much Reading and Writing):**\n",
        "      In MapReduce, after every step of processing (map or reduce), the results are written to the hard disk and then read back again for the next step. Imagine writing your notes on paper after every single thought and then reading them back before continuing — it would take much longer than just keeping them in memory. This constant saving and loading makes the process very slow, especially when the job has many steps or loops.\n",
        "\n",
        "  * **High Latency (Slow Responses):**\n",
        "      Because MapReduce depends so heavily on reading and writing to disk, it cannot respond quickly. It’s like waiting for a slow delivery service instead of picking up something instantly. For applications that need quick answers, like interactive data queries or real-time dashboards, this delay becomes a serious problem.\n",
        "\n",
        "  * **Rigid Programming Model (Lack of Flexibility):**\n",
        "      MapReduce forces you to write programs only in terms of two operations: **map** (transform data) and **reduce** (combine data). While this works for many simple tasks, it becomes awkward and complicated when you want to perform advanced analytics or iterative algorithms like machine learning. It’s like being asked to build every piece of furniture using only a hammer and nails — possible, but not efficient or practical.\n",
        "\n",
        "\n",
        "*   **Spark as a fast, general-purpose cluster computing framework:** Spark addresses these limitations by emphasizing in-memory computation. It can process data orders of magnitude faster than MapReduce for many workloads because it keeps data in RAM whenever possible, drastically reducing disk I/O.\n",
        "\n",
        "**Core Features of Spark:**\n",
        "\n",
        "Spark's power and versatility come from its core features:\n",
        "\n",
        "*   **In-Memory Computation:** This is Spark's most significant differentiator. By caching data in memory across iterations and stages, Spark dramatically accelerates processing, particularly for iterative algorithms like those used in machine learning and graph processing.\n",
        "*   **Distributed Computing Model:** Spark operates on clusters of machines, distributing data and computation to process large datasets in parallel.\n",
        "*   **Unified Stack (SQL, MLlib, GraphX, Streaming):** Spark provides a comprehensive set of high-level libraries built on the core Spark engine:\n",
        "    *   **Spark SQL:** For structured data processing using SQL or the DataFrame API.\n",
        "    *   **MLlib:** A machine learning library with a wide range of algorithms.\n",
        "    *   **GraphX:** For graph computation.\n",
        "    *   **Spark Streaming:** For processing live data streams.\n",
        "    This unified stack allows developers to use a single framework for various big data tasks.\n",
        "*   **Use Cases:** Spark is widely used across industries for various applications:\n",
        "    *   **ETL Pipelines:** Extracting, transforming, and loading data from various sources.\n",
        "    *   **Real-Time Analytics:** Processing streaming data for immediate insights.\n",
        "    *   **ML Pipelines:** Building and deploying machine learning models on large datasets.\n",
        "    *   Graph processing, interactive data analysis, and more.\n",
        "\n",
        "### Spark Ecosystem diagram\n",
        "\n",
        "\n",
        "  <img align=\"center\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Spark_ecosystem_decisions.png?raw=true\"  width=\"500\"/>\n",
        "\n",
        "\n",
        "\n",
        "The Apache Spark ecosystem is best understood as a **layered architecture**, where each layer has a specific role but also connects smoothly with the others. At the very top, Spark provides **APIs in different languages** such as Scala, Python (PySpark), Java, and R, allowing developers and analysts to interact with Spark in the language they are most comfortable with. These APIs are the entry point for users to write applications that run on Spark.\n",
        "\n",
        "Beneath the APIs, Spark is divided into its **core modules**, which handle different types of data processing. **Spark SQL** manages structured data using SQL queries and DataFrames. **Structured Streaming** supports real-time stream processing using the same DataFrame and SQL API, making batch and streaming processing feel unified. **Spark MLlib** and the newer **spark.ml pipelines** provide machine learning functionality, from classification and regression to advanced ML pipelines. **GraphX** handles graph-based analytics, such as social network analysis or relationship modeling. All of these modules are built on top of Spark’s **Core engine**, which manages distributed task scheduling, fault tolerance, and the underlying abstraction of data (RDDs). This Core is the “heart” of Spark, ensuring computations are spread across multiple machines and results are combined efficiently.\n",
        "\n",
        "Supporting the core modules, Spark connects to a **storage layer** where data resides. This can be Hadoop’s HDFS, Amazon S3, Azure Data Lake, local file systems, or Delta Lake. Delta Lake is often highlighted because it provides reliability features like ACID transactions and schema enforcement on top of cloud storage, making Spark jobs more dependable. Data can be batch-loaded from these stores or streamed continuously in case of live applications.\n",
        "\n",
        "To manage how Spark jobs are distributed across machines, Spark relies on a **cluster manager layer**. Common options include **YARN** (from the Hadoop ecosystem), **Apache Mesos**, **Kubernetes**, or Spark’s **Standalone cluster manager**. The cluster manager’s role is to allocate resources (CPU, memory) to Spark executors running on the worker nodes. This layer sits between the raw infrastructure and Spark itself, ensuring efficient scaling.\n",
        "\n",
        "At the bottom is the **infrastructure layer**, which consists of the actual physical or virtual machines where Spark runs. These could be on-premises servers or cloud resources like **Amazon EC2 instances**, Google Cloud Compute, or Azure VMs. In the cloud, Spark can also run via managed services such as Amazon EMR or Databricks, which simplify cluster setup.\n",
        "\n",
        "Finally, around this Spark core, there are important **ecosystem tools** that extend its power. **MLflow** integrates with Spark for managing the machine learning lifecycle (experiment tracking, model management, deployment). **Spark Packages** provide a wide variety of third-party community-built connectors and libraries to extend Spark’s capabilities (e.g., connectors for NoSQL databases, graph algorithms). Architectural patterns like **Lambda and Kappa architectures** are not part of Spark itself but describe ways to design big data pipelines that often make heavy use of Spark modules.\n",
        "\n",
        "Together, these layers form a connected system: users write code in APIs → which call Spark Core and modules → which read/write from storage → and run jobs distributed by cluster managers → across infrastructure. The ecosystem tools enhance workflows by adding reliability, lifecycle management, or external integrations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Core Concepts"
      ],
      "metadata": {
        "id": "3qqkr1Esbpgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Core is the foundational execution engine of Apache Spark, providing core functionalities like task scheduling, memory management, and fault recovery. It introduces the concept of the Resilient Distributed Dataset (RDD), which is the primary, low-level data abstraction in Spark. Ultimately, all operations and computations defined on RDDs are translated into tasks that Spark Core schedules and executes across the cluster."
      ],
      "metadata": {
        "id": "DEKjkfcab8MC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resilient Distributed Dataset (RDD) in Spark"
      ],
      "metadata": {
        "id": "opwHZKxztY9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/RDD_Properties_.png?raw=true\"  width=\"700\"/>\n",
        "\n",
        "In Apache Spark, the **Resilient Distributed Dataset (RDD)** is the fundamental data structure, and you can think of it as the core engine that makes Spark powerful. At its simplest, an RDD is just a collection of data, like a list in Python, but with two important differences: it is **distributed across multiple machines in a cluster**, and it is **fault-tolerant**, meaning that even if some part of the data is lost due to machine failure, Spark can automatically rebuild it using the instructions (or lineage) it has stored. This is why it’s called “resilient.” For example, if you load a huge dataset from HDFS or a database into Spark, the data is broken into small chunks and spread across different machines in the cluster. The RDD acts as a handle to manage this distributed collection, letting you apply operations on it as if you were working with a single dataset on your own computer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_YxKndjTtb_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()  # sc is the SparkContext, the entry point to Spark\n",
        "rdd = sc.parallelize(range(10,20))  # Create RDD with numbers from 10 to 19\n",
        "print(rdd)\n",
        "rdd.collect()  # Collects all data from nodes and returns it to the driver node"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxZDxsILIFa4",
        "outputId": "f0975d38-5958-4b8a-ea3d-c46a51558f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[1] at RDD at PythonRDD.scala:53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()  # sc is the SparkContext, the entry point to Spark\n",
        "rdd = sc.parallelize(range(10,20))  # Create RDD with numbers from 10 to 19\n",
        "print(rdd)\n",
        "rdd.collect()  # Collects all data from nodes and returns it to the driver node\n",
        "```\n",
        "```\n",
        "PythonRDD[4] at RDD at PythonRDD.scala:53\n",
        "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
        "```\n",
        "#### Explanation:\n",
        "\n",
        "* **`from pyspark import SparkContext`**: Imports the `SparkContext` class, which is the main entry point to Spark functionality. Without this, you cannot create RDDs or interact with the Spark cluster.\n",
        "\n",
        "* **`sc = SparkContext.getOrCreate()`**: This line either creates a new `SparkContext` or retrieves the existing one if it’s already running. The `SparkContext` is the connection between your Python program and the Spark cluster (or your local Spark session if no cluster is specified).\n",
        "\n",
        "* **`rdd = sc.parallelize(range(10,20))`**: Creates an RDD (Resilient Distributed Dataset) from the numbers 10 to 19. The function `parallelize()` splits the given collection into partitions, which can then be distributed across multiple machines or cores for parallel processing.\n",
        "\n",
        "* **`print(rdd)`**: Printing the RDD itself will not display its actual data. Instead, it shows the RDD object reference and metadata (like its ID and location in the DAG).\n",
        "\n",
        "* **`rdd.collect()`**: This is an **action** that gathers all the elements of the RDD from the distributed cluster and returns them as a list to the driver program. Since collecting data brings everything into the driver’s memory, it should be used carefully for small datasets only.\n",
        "\n"
      ],
      "metadata": {
        "id": "U4NPd2vEPhlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print(rdd.glom().collect())\n",
        "print(\"Number of partitions:\", rdd.getNumPartitions())\n",
        "```\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Rdd_Split.png?raw=true\"  width=\"300\"/>\n",
        "\n",
        "#### Explanation:\n",
        "\n",
        "* **`rdd.glom().collect()`**:\n",
        "\n",
        "  * The `glom()` transformation groups all elements within each partition into a list, creating a new RDD where each element is one list per partition.\n",
        "  * When you call `.collect()`, Spark brings these lists back to the driver as a list of lists. This way, you can clearly see how your dataset has been split across partitions.\n",
        "  * For example, if your RDD has 2 partitions for numbers `10–19`, you might see something like:\n",
        "\n",
        "    ```\n",
        "    [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]\n",
        "    ```\n",
        "\n",
        "    showing exactly how the data is distributed.\n",
        "\n",
        "* **`rdd.getNumPartitions()`**:\n",
        "\n",
        "  * This method returns the total number of partitions in the RDD.\n",
        "  * Partitions are the basic units of parallelism in Spark: each partition can be processed independently on different cores or machines.\n",
        "  * Knowing the number of partitions helps you understand how Spark has divided your data, which affects both parallel performance and resource usage.\n",
        "  * For example, if it prints:\n",
        "\n",
        "    ```\n",
        "    Number of partitions: 2\n",
        "    ```\n",
        "\n",
        "    it means Spark split your RDD into 2 chunks that can be processed in parallel.\n"
      ],
      "metadata": {
        "id": "dJqzuTwiQ3xB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How RDDs work"
      ],
      "metadata": {
        "id": "18H51IP81bT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **working of RDDs** is centered around two types of operations: *transformations* and *actions*. Transformations are operations that describe how you want to change your data, such as filtering out certain values, mapping each record to a new form, or grouping them by a key. These transformations don’t immediately do the work; instead, they create a new RDD that remembers the transformation that should be applied to the original data. Actions, on the other hand, are operations that actually trigger computation, like counting the elements, collecting results back to the driver, or writing them to a file. This design is crucial because it allows Spark to delay execution until it really knows what you want to do with the data.\n",
        "\n",
        "This leads to the concept of **lazy evaluation**, which is one of the most important features of RDDs. Because of lazy evaluation, when you apply transformations on an RDD, Spark doesn’t execute them immediately but simply records the steps. These recorded steps form the **lineage** of the RDD, which is like a recipe showing how the data should be derived from its source. Spark represents this lineage internally as a **Directed Acyclic Graph (DAG)**, where each node is an RDD and each edge is a transformation.\n",
        "\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/RDD_Operations.png?raw=true\"  width=\"400\"/>\n",
        "\n",
        "\n",
        "\n",
        "The DAG is like a roadmap of how your data will flow from its original source through the transformations you’ve defined, step by step, until it reaches the final output. “Directed” means the steps have a direction (you can only go forward, from input to output), and “Acyclic” means there are no loops (you can’t come back to a previous step). For example, if you read data, then map it, then filter it, Spark draws a DAG where each node is an RDD and each edge is the transformation connecting them. When you finally call an action, Spark looks at this DAG, breaks it into stages of tasks that can be run in parallel, and executes them on the cluster. At this stage, Spark takes advantage of in-memory computation, meaning intermediate results of these stages can be kept in RAM instead of writing to disk after each step. This makes iterative operations and repeated actions on the same data much faster, as Spark can reuse the cached results directly from memory while still relying on the DAG for recovery if any partition is lost. At this point, you may note that the RDD is **immutable** — once created it cannot be modified. This immutability ensures that each transformation always produces a new RDD, which naturally avoids loops in the DAG and makes fault recovery and parallel execution much simpler.\n",
        "\n",
        "#### Example\n",
        "\n",
        "\n",
        "\n",
        "### Code Explanation\n",
        "\n",
        "```python\n",
        "filteredRDD = rdd.filter(lambda x: x % 2 == 0)\n",
        "```\n",
        "\n",
        "* Applies a **filter transformation** to keep only even numbers. This doesn’t run immediately (lazy evaluation), it just records the operation.\n",
        "\n",
        "```python\n",
        "print(filteredRDD.toDebugString())\n",
        "```\n",
        "\n",
        "* Prints the **lineage (DAG)** of the RDD, showing how `filteredRDD` is derived from the original `ParallelCollectionRDD`. This helps Spark recompute data if needed.\n",
        "\n",
        "```python\n",
        "filteredRDD.collect()\n",
        "```\n",
        "\n",
        "* An **action** that triggers execution. Spark filters each partition locally (no shuffle) and returns the even numbers as a list: `[10, 12, 14, 16, 18]`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yvoIvU_T1cVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filteredRDD = rdd.filter(lambda x: x % 2 == 0)\n",
        "print(filteredRDD.toDebugString())\n",
        "filteredRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUn9Jp2dtlQE",
        "outputId": "bf0c171c-f254-464a-d1a0-bcc586d9e52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'(2) PythonRDD[2] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10, 12, 14, 16, 18]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### RDD Transformations vs. Actions\n",
        "\n",
        "| **Transformations** (return a new RDD, lazy) | **Description**                                               | **Actions** (return a value or output) | **Description**                                                    |\n",
        "| -------------------------------------------- | ------------------------------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| **map(func)**                                | Applies a function to each element and returns a new RDD.     | **collect()**                                             | Returns all elements of the RDD to the driver as a list.           |\n",
        "| **filter(func)**                             | Keeps elements that satisfy the condition.                    | **count()**                                               | Returns the number of elements in the RDD.                         |\n",
        "| **flatMap(func)**                            | Like `map`, but can return multiple values for each input.    | **first()**                                               | Returns the first element of the RDD.                              |\n",
        "| **mapPartitions(func)**                      | Applies a function to each partition instead of each element. | **take(n)**                                               | Returns the first `n` elements.                                    |\n",
        "| **union(rdd)**                               | Returns an RDD containing elements from both RDDs.            | **reduce(func)**                                          | Aggregates elements using a function (e.g., sum).                  |\n",
        "| **intersection(rdd)**                        | Returns elements present in both RDDs.                        | **collectAsMap()**                                        | Returns the key-value pairs (for pair RDDs) as a dictionary.       |\n",
        "| **distinct()**                               | Removes duplicates.                                           | **saveAsTextFile(path)**                                  | Saves the RDD as a text file.                                      |\n",
        "| **sample(withReplacement, fraction)**        | Returns a sample subset of the RDD.                           | **saveAsSequenceFile(path)**                              | Saves the RDD as a Hadoop sequence file.                           |\n",
        "| **groupByKey()** (pair RDD)                  | Groups values by key.                                         | **foreach(func)**                                         | Runs a function on each element (no return).                       |\n",
        "| **reduceByKey(func)** (pair RDD)             | Merges values with the same key using a function.             | **takeSample(withReplacement, n)**                        | Returns a random sample of `n` elements.                           |\n",
        "| **sortByKey()** (pair RDD)                   | Returns an RDD sorted by keys.                                | **countByKey()**                                          | Returns a map of keys and their counts.                            |\n",
        "| **join(rdd)** (pair RDD)                     | Joins two RDDs by key.                                        | **lookup(key)**                                           | Returns all values associated with the given key.                  |\n",
        "| **cartesian(rdd)**                           | Returns all pairs of elements from two RDDs.                  | **reduceByKeyLocally(func)**                              | Returns the result of `reduceByKey` as a dictionary on the driver. |\n"
      ],
      "metadata": {
        "id": "ts7f0AP06EGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spark Architecture"
      ],
      "metadata": {
        "id": "rDnlRFCu6m53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Spark_Architecture.png?raw=true\"  width=\"400\"/>\n",
        "  \n",
        "The **Spark Driver** is at the heart of every Spark application. You can think of it as the “brain” of Spark. It runs the code that you write in Python, Java, or Scala, and it is responsible for turning your high-level operations into something Spark can actually execute across the cluster. Inside the driver, a special object called the **SparkContext** is created. This SparkContext is like the steering wheel of the entire application: it connects your program to the Spark cluster, requests resources, and manages the execution of jobs. The driver also keeps track of metadata, schedules tasks, and finally collects results back to your program. Without the driver, your Spark application has no one to coordinate or give instructions.\n",
        "\n",
        "Next, there is the **Cluster Manager**, which acts as the “resource allocator” for Spark. Imagine you are working in a big computer lab with many machines. The cluster manager is like the lab supervisor who decides which machines are free and how much memory and processing power each Spark application should get. Spark can work with different cluster managers such as its own **Standalone cluster manager**, **YARN** (from Hadoop), or **Mesos**. The driver doesn’t directly grab resources from the cluster; instead, it sends a request to the cluster manager, and the cluster manager decides where to launch executors on the worker nodes.\n",
        "\n",
        "The real computation happens on the **Worker Nodes**. These are the actual physical or virtual machines in the cluster that carry out the work. Each worker runs one or more **Executor** processes. Executors are long-running JVM (Java Virtual Machine) processes that are responsible for executing the tasks assigned by the driver. They also store data in memory (or on disk if memory is full) for faster access in subsequent operations. Executors are critical because they do two jobs: they **run the tasks** and they **hold the data** that those tasks operate on. If you imagine the driver as a teacher giving out homework, then the executors are the students actually solving the problems.\n",
        "\n",
        "Each executor runs multiple **Tasks**, which are the smallest units of work in Spark. A task usually corresponds to performing an operation on a single partition of the RDD or DataFrame. Since a dataset is divided into partitions, multiple tasks can run in parallel, each handling a different slice of the data. Tasks are sent by the driver to the executors, and once executed, the results are either stored for reuse or returned back to the driver. This task-based parallelism is what makes Spark so fast and scalable.\n",
        "\n",
        "Finally, Spark needs a place to read and write data, and this is where systems like **HDFS (Hadoop Distributed File System)** or other storage layers (like Amazon S3, local file systems, or relational databases) come in. Spark workers fetch chunks of data from these storage systems and process them in parallel. HDFS or similar systems ensure the data is distributed and fault-tolerant, matching Spark’s own design philosophy.\n",
        "\n"
      ],
      "metadata": {
        "id": "3W-Dxc2v6zoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution Flow of a Spark Job"
      ],
      "metadata": {
        "id": "NpQ5B4Z-Th3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  <img align=\"center\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Execution_Flow_of_a_Spark_Job.png?raw=true\"  width=\"700\"/>\n",
        "\n",
        "**1. Launching the Spark Driver**\n",
        "\n",
        "The Spark Driver is the central process that coordinates your entire Spark application. Who launches it depends on how you submit your job. If you are running your code interactively (e.g., from an IDE like PyCharm or a Jupyter notebook on your local machine), your local computer itself acts as the Driver and launches the process. If you submit your compiled application JAR (for Scala/Java) or Python file to a cluster using the `spark-submit` script, the `spark-submit` command is responsible for launching the Driver program on either your local machine (in \"client\" deploy mode) or on one of the worker nodes inside the cluster (in \"cluster\" deploy mode).\n",
        "\n",
        "**2. The Role of SparkContext and Initial Setup**\n",
        "\n",
        "Once the Driver process starts, the very first thing it does is execute your code. The first crucial line is the creation of a **SparkContext** (or a **SparkSession**, which is a modern wrapper around SparkContext). Think of the SparkContext as the heart of your application—it's the object that represents the connection to a Spark cluster. It tells your Driver program *how* to talk to the cluster. During its creation, it communicates with the **Cluster Manager** (like YARN, Mesos, or Spark's own standalone manager) to negotiate resources. The Driver asks the Cluster Manager for executors (worker processes) to be launched on the cluster's worker nodes. The Cluster Manager then launches these executors and reports back to the Driver with information about where they are located.\n",
        "\n",
        "**3. Building the Computational Plan: DAG and Lazy Evaluation**\n",
        "\n",
        "As your Driver program continues to run, it defines a series of operations on your data. When you create an RDD from a file or a DataFrame from a table, and then apply **transformations** (like `map`, `filter`, `join`), a very important thing happens: *nothing is computed yet*. Spark does not read the data or perform these operations immediately. This is called **lazy evaluation**. Instead, Spark internally starts building a graph of these operations, recording what needs to be done. This graph is called a **Directed Acyclic Graph (DAG)**. The **DAG Scheduler** (a component inside the SparkContext) is responsible for taking this sequence of RDD transformations and breaking it down into stages of tasks that can be executed in parallel. It's like a master architect drawing up a detailed, step-by-step blueprint for building a house, but without actually laying a single brick.\n",
        "\n",
        "**4. Triggering Execution with an Action**\n",
        "\n",
        "The entire computational plan defined by the DAG remains dormant until you call an **action**. Actions are operations like `count()`, `collect()`, `saveAsTextFile()`, or `show()` that require a concrete result to be returned to the Driver program or written to storage. When an action is called, it forces the DAG Scheduler to finalize the DAG and hand it over to the **Task Scheduler**. The Task Scheduler is the one that talks to the Cluster Manager to know which executors are available. It then breaks the DAG's stages into individual **tasks** and sends these tasks to the executors (the workers) across the cluster. The executors finally perform the actual work: they read the data, apply all the transformations in the plan, compute the result for their partition of the data, and either send the final result back to the Driver or write it out to the specified location.\n",
        "\n",
        "**5. Shutting Down the Session**\n",
        "\n",
        "After all the actions in your application have been completed and you have your results, it is good practice to gracefully shut down the Spark cluster connection by calling `spark.stop()` or `sc.stop()`. This command tells the Driver to communicate with the Cluster Manager, which then shuts down all the executors it allocated for this job and releases the computing resources (CPU, memory) back to the cluster. This ensures resources are available for other users and applications, preventing resource leaks.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4MhVWfzcSOo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference"
      ],
      "metadata": {
        "id": "DUtQ5oTBbftX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. https://luminousmen.com/post/spark-core-concepts-explained/\n",
        "2. https://spark.apache.org/docs/latest/rdd-programming-guide.html#linking-with-spark\n",
        "3. https://medium.com/@sangee01sankar17/apache-spark-rdd-a0cd391e55d4\n",
        "4. https://rockybhatia.substack.com/p/apache-spark-explained-architecture\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KfaGYFS6Nn1a"
      }
    }
  ]
}