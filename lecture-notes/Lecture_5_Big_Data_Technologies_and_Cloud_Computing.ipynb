{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<body>\n",
        "  <center>\n",
        "    <img align=\"center\" src=\"https://duk.ac.in/wp-content/uploads/2021/03/logo_transparent_bg_caption.png\" width=\"200px\" padding=\"10px\"   style=\" width:300px; padding: 10px;  \" />\n",
        "  <div class=\"university-details\">\n",
        "    <h2> </h2>\n",
        "    <center><h2>Kerala University of Digital Sciences, Innovation and Technology</h2></center> <h3>(Digital University Kerala, DUK)</h3>\n",
        "    <h4><i><b>School of Digital Sciences</b></i><br></h4>\n",
        "  </div>\n",
        "  <h1>  </h1>\n",
        "  </center>\n",
        "  <h5><b>Date:</b> 04/08/2025 <br>\n",
        "<b>Venue:</b> Room Number 45<br>\n",
        "<b>Topic:</b> Spark DataFrames<br>\n",
        "<b>Instructor:</b> Dr. Aswin VS</h5>\n",
        "</body>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zF-mY4W5Tbli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark DataFrames**"
      ],
      "metadata": {
        "id": "n1LFyQ7Fk0nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction to Spark DataFrames**"
      ],
      "metadata": {
        "id": "Y9IRewZ0lG35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "A **Spark DataFrame** is a distributed collection of data organized into rows and columns, much like a table in a relational database or a spreadsheet. It is one of the most widely used data structures in Apache Spark because it allows you to work with large datasets in an intuitive way using familiar operations such as selecting columns, filtering rows, and aggregating results. DataFrames are built on top of **RDDs (Resilient Distributed Datasets)**, which are Spark’s core abstraction for distributed data. While RDDs give you fine-grained control, they can be low-level and harder to optimize. DataFrames, on the other hand, provide a higher-level abstraction that makes it easier to write clean code and benefit from Spark’s internal optimizations.\n",
        "\n",
        "If you are already familiar with **pandas DataFrames** in Python, Spark DataFrames feel very similar. Both are used for handling tabular data, support column operations, and offer functions for filtering, grouping, and aggregating. The key difference is that pandas DataFrames work on a **single machine**, so they are limited by the memory of that machine, while Spark DataFrames are **distributed across a cluster**, which means they can handle much larger datasets that do not fit into a single computer’s memory. Also, Spark executes operations in a distributed and parallel way, which makes it suitable for big data processing.\n",
        "\n",
        "There are several benefits of using Spark DataFrames. First, they enable **distributed processing**, which means the data is automatically divided across multiple machines in a cluster and processed in parallel. Second, they support **optimized execution** using Spark’s **Catalyst optimizer** (which analyzes queries and finds the most efficient execution plan) and **Tungsten engine** (which improves memory and CPU efficiency). Another important concept is **lazy evaluation**, meaning that when you write a series of transformations on a DataFrame, Spark does not execute them immediately. Instead, it builds a logical plan and only performs the computation when an action like `show()` or `collect()` is called. This allows Spark to optimize the entire workflow. Finally, Spark DataFrames are designed to **support very large-scale data**, making it possible to analyze terabytes or even petabytes of data efficiently, something not possible with traditional tools like pandas alone.\n",
        "\n",
        "  <img align=\"center\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/DataFrame_API.png?raw=true\"  width=\"500\"/>\n",
        "\n",
        "In Apache Spark, the **RDD (Resilient Distributed Dataset) API** forms the foundation and is part of **Spark Core**, which provides basic distributed computing functions like `map()`, `filter()`, and `reduceByKey()`. On top of this, the **DataFrame API** is introduced as part of **Spark SQL**, offering a higher-level abstraction for structured data with familiar operations such as `select()`, `groupBy()`, and `join()`. While the DataFrame API belongs to Spark SQL, other modules—such as **Structured Streaming**, **Spark ML**, and **GraphX/GraphFrames**—extend and reuse this abstraction in their own domains. Structured Streaming treats live data as an evolving DataFrame, Spark ML uses DataFrames to represent features and labels for machine learning pipelines, and GraphFrames represent vertices and edges as DataFrames to support graph algorithms. This makes DataFrames the **unifying abstraction** in Spark for working with structured data, while RDDs in Spark Core remain the low-level building block for distributed computation.\n"
      ],
      "metadata": {
        "id": "7vgrZ8aylBxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PySpark DataFrame Cheat Sheet**"
      ],
      "metadata": {
        "id": "S63YNja0d--8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark\n",
        "spark = SparkSession.builder.appName(\"DF_Creation\").getOrCreate()"
      ],
      "metadata": {
        "id": "uLmtNODIhUUp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating DataFrames"
      ],
      "metadata": {
        "id": "kROGbSh8fVi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From list of tuples\n",
        "\n",
        " Use `createDataFrame(data, schema)` where `data` is a list/tuple and `schema` defines column names."
      ],
      "metadata": {
        "id": "63mBgUhEjJTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From list of tuples\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 28)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP63WA4phiz5",
        "outputId": "434b98f6-0ac7-44f9-9c1e-0a5187cb1872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From RDD\n",
        "\n",
        "RDDs can be converted to DataFrames using `.toDF(schema)` for structured operations."
      ],
      "metadata": {
        "id": "dEls9uGVi2mY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert RDD to DataFrame\n",
        "rdd = spark.sparkContext.parallelize([(\"John\", 35), (\"Daisy\", 40)])\n",
        "df = rdd.toDF([\"Name\", \"Age\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XSUV1uyiuGn",
        "outputId": "2e7d89e5-4873-4e55-e8c5-176e0f5968ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "| John| 35|\n",
            "|Daisy| 40|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From External Files\n",
        "\n",
        "Spark automatically understands these file formats and can infer schema."
      ],
      "metadata": {
        "id": "9vPwOti_nIlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# csv file\n",
        "df_csv = spark.read.csv(\"Customers.csv\", header=True, inferSchema=True)\n",
        "df_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkraOMy3nc9m",
        "outputId": "08369017-dc1b-48b3-b0f6-f963dad4c2a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+---+-----------------+--------------+\n",
            "|_c0|CustomerID|Gender|Age|Annual Income(k$)|Spending Score|\n",
            "+---+----------+------+---+-----------------+--------------+\n",
            "|  0|         1|  Male| 20|               10|            30|\n",
            "|  1|         2|Female| 21|               20|            50|\n",
            "|  2|         3|Female| 19|               30|            48|\n",
            "|  3|         4|  Male| 18|               10|            84|\n",
            "|  4|         5|  Male| 25|               25|            90|\n",
            "|  5|         6|Female| 26|               60|            65|\n",
            "|  6|         7|  Male| 32|               70|            32|\n",
            "|  7|         8|  Male| 41|               15|            46|\n",
            "|  8|         9|Female| 20|               21|            12|\n",
            "|  9|        10|  Male| 19|               22|            56|\n",
            "+---+----------+------+---+-----------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from jason\n",
        "df = spark.read.json(\"Student.json\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5awGktKXypvN",
        "outputId": "5f89a936-15dc-4298-f128-b5b41d735916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----+-----------+----------------+--------------------+----+\n",
            "|          department|               email|  id|       name|placement_status|              skills|year|\n",
            "+--------------------+--------------------+----+-----------+----------------+--------------------+----+\n",
            "|    Computer Science|ananya.rao@exampl...|S101| Ananya Rao|        Eligible|[Python, Data Ana...|   3|\n",
            "|         Electronics|rohan.mehta@examp...|S102|Rohan Mehta|          Placed|[Embedded Systems...|   4|\n",
            "|Information Techn...|meera.nair@exampl...|S103| Meera Nair|    Not Eligible|[HTML, CSS, JavaS...|   2|\n",
            "+--------------------+--------------------+----+-----------+----------------+--------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from  Parquet\n",
        "df_parquet = spark.read.parquet(\"student_placement.parquet\")\n",
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMXBtXyHtNkf",
        "outputId": "229158b4-41eb-4879-ee68-f82d1051b6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+----------+----+--------------------+----------------+---------------+\n",
            "|student_id|          name|department|cgpa|              skills|placement_status|interview_score|\n",
            "+----------+--------------+----------+----+--------------------+----------------+---------------+\n",
            "|      S201|   Divya Menon|        IT| 8.2|       [Python, SQL]|        Eligible|           NULL|\n",
            "|      S202|    Arjun Nair|        CS| 9.1|  [Java, Git, Cloud]|          Placed|           85.0|\n",
            "|      S203|  Sneha Pillai|       ECE| 7.8|      [MATLAB, VHDL]|    Not Eligible|           NULL|\n",
            "|      S204|Rahul Krishnan|        ME| 8.5|[AutoCAD, SolidWo...|        Eligible|           78.0|\n",
            "+----------+--------------+----------+----+--------------------+----------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark Schema & Data Types"
      ],
      "metadata": {
        "id": "zrweAJUkzUnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Inferred Schema**: Spark automatically determines the data types of columns by sampling the data, rather than requiring the user to explicitly define the schema"
      ],
      "metadata": {
        "id": "mD4o-sEzn5DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePwguiCUnxQG",
        "outputId": "0d9cde2b-3fbe-48d6-dd38-50baa2bb137f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`printSchema()` is a PySpark DataFrame method that displays the schema in a tree format, showing all column names and their inferred or explicitly defined data types."
      ],
      "metadata": {
        "id": "hw_K049UobhW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Explicit Schema**: An explicit schema is a user-defined StructType that precisely specifies the name, data type, and nullability of each column within a DataFrame, serving as a contract for data ingestion that ensures type safety and enhances performance by avoiding the need for a costly inference pass."
      ],
      "metadata": {
        "id": "lc-TVJgmof6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame([(\"Alice\", 25), (\"Bob\", 30)], schema)\n",
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nfhHxRTpESt",
        "outputId": "4d516fdd-fbaa-4bda-86ed-bcb88c2787ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PySpark, **\"nullable = true\"** in a DataFrame's `printSchema()` output means that a column is allowed to contain **`null` values**. Conversely, if it says **`\"nullable = false\"`**, it means that column is **not allowed** to have `null` values and Spark will throw an error if it encounters one."
      ],
      "metadata": {
        "id": "xI3JC9rBpD-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Common Data Types**\n",
        "\n",
        "| Data Type | Description | PySpark Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`StringType`** | Represents character string values (text). | `StringType()` |\n",
        "| **`IntegerType`** | Represents 4-byte signed integer values (whole numbers). | `IntegerType()` |\n",
        "| **`DoubleType`** | Represents 8-byte double-precision floating-point numbers (decimal numbers). | `DoubleType()` |\n",
        "| **`BooleanType`** | Represents boolean values, either `true` or `false`. | `BooleanType()` |\n",
        "| **`StructType`** | Represents a structured type, used for nested data. | `StructType([])` |"
      ],
      "metadata": {
        "id": "ANRitEpypymF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType, IntegerType, DoubleType, BooleanType, StructType\n",
        "\n",
        "# Examples\n",
        "StringType()   # Text data\n",
        "IntegerType()  # Whole numbers\n",
        "DoubleType()   # Decimal numbers\n",
        "BooleanType()  # True/False\n",
        "StructType([]) # Nested structure\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x09NUQpvplMd",
        "outputId": "c7666003-4d88-43ac-ecf6-6541107c9d0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame Basic Operations"
      ],
      "metadata": {
        "id": "kuXbAo0qqeOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DF_Basics\").getOrCreate()\n",
        "data = [(\"Alice\", 25, \"HR\"), (\"Bob\", 30, \"IT\"), (\"Cathy\", 28, \"Finance\")]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Dept\"])\n"
      ],
      "metadata": {
        "id": "NCP-kVW_rTXl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Viewing Data** is used to inspect its contents to verify data\n",
        "| Method | Description |\n",
        "| :--- | :--- |\n",
        "| `df.show()` | Displays the top rows in a tabular format. |\n",
        "| `df.take(n)` | Returns the first `n` rows as a list of `Row` objects. |\n",
        "| `df.collect()` | Returns all rows as a list of `Row` objects. Use this with caution on large datasets as it loads all data into memory. |"
      ],
      "metadata": {
        "id": "ZZsseT82qhis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()          # Display top rows in tabular format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cfvRXQDrZhl",
        "outputId": "56d7c99d-750e-466c-e837-81bea08b9eb4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name|Age|   Dept|\n",
            "+-----+---+-------+\n",
            "|Alice| 25|     HR|\n",
            "|  Bob| 30|     IT|\n",
            "|Cathy| 28|Finance|\n",
            "+-----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(2)         # Return first 2 rows as list of Row objects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa6xEbuvruBm",
        "outputId": "ccfad401-3fd1-499a-862d-32978041b639"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Name='Alice', Age=25, Dept='HR'), Row(Name='Bob', Age=30, Dept='IT')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.collect()       # Return all rows (use carefully on big data!)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY42zanCrkTF",
        "outputId": "4744f381-3a27-47f5-ef44-252f6aa9392c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(Name='Alice', Age=25, Dept='HR'),\n",
              " Row(Name='Bob', Age=30, Dept='IT'),\n",
              " Row(Name='Cathy', Age=28, Dept='Finance')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Selecting Columns** used for choosing a specific subset of columns to work with, rather than using the entire dataset.\n",
        "\n",
        "* The select() function is used to choose and return a new DataFrame containing a specified subset of columns."
      ],
      "metadata": {
        "id": "8eTRJdFor3h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Name\", \"Age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50IytF-0sUU1",
        "outputId": "e01106a9-b538-49ed-c9fb-726c36699987"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 28|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(col(\"Dept\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gvIS9vlsX68",
        "outputId": "ecd87042-09d7-4d21-bc2f-2daddd5a8ea5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   Dept|\n",
            "+-------+\n",
            "|     HR|\n",
            "|     IT|\n",
            "|Finance|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Filtering Rows** is used for selecting a subset of a DataFrame's rows based on one or more ***conditions***.\n",
        "\n",
        "| Function | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`filter()`** | Used to filter rows based on a given condition. It's often considered more readable and is a direct alias for the `where()` function. | `df.filter(\"age > 30\")` |\n",
        "| **`where()`** | Also used to filter rows based on a given condition. It is a direct alias for the `filter()` function, meaning they are functionally identical and can be used interchangeably. | `df.where(\"city == 'New York'\")` |"
      ],
      "metadata": {
        "id": "wyIws9zwsd9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.Age > 28).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEnHM12WstfG",
        "outputId": "7d9f7ee7-4db2-4c1f-869b-cdaf9d317629"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----+\n",
            "|Name|Age|Dept|\n",
            "+----+---+----+\n",
            "| Bob| 30|  IT|\n",
            "+----+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.where(col(\"Dept\") == \"IT\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO1CZQPptLuF",
        "outputId": "aef14b1d-3729-42b9-9d58-d6b992f2b38b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+----+\n",
            "|Name|Age|Dept|\n",
            "+----+---+----+\n",
            "| Bob| 30|  IT|\n",
            "+----+---+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Adding New Columns** is the process of creating a new column in a DataFrame, often by performing a calculation or transformation on existing columns.\n",
        "\n",
        "The `withColumn()` function is used to add a new column to a DataFrame or to replace an existing column with a new one."
      ],
      "metadata": {
        "id": "MvLWgbwttQ_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"AgeAfter5Years\", df.Age + 5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ_7iWpUteG2",
        "outputId": "31f2505d-c5e2-45e6-8f39-bc0871c4146c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+--------------+\n",
            "| Name|Age|   Dept|AgeAfter5Years|\n",
            "+-----+---+-------+--------------+\n",
            "|Alice| 25|     HR|            30|\n",
            "|  Bob| 30|     IT|            35|\n",
            "|Cathy| 28|Finance|            33|\n",
            "+-----+---+-------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Renaming / Dropping Columns** involves changing a column's name or removing it entirely from a DataFrame.\n",
        "\n",
        "| Function | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`withColumnRenamed()`** | Used to rename a single column in a DataFrame without changing its data or position. | `df.withColumnRenamed(\"old_name\", \"new_name\")` |\n",
        "| **`drop()`** | Used to remove one or more specified columns from a DataFrame. | `df.drop(\"column1\", \"column2\")` |\n"
      ],
      "metadata": {
        "id": "xTa-eQIXtpCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumnRenamed(\"Dept\", \"Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uyicDWmt5L1",
        "outputId": "be3f41ad-5e98-4226-d4a4-6a5c07fa28e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+----------+\n",
            "| Name|Age|Department|\n",
            "+-----+---+----------+\n",
            "|Alice| 25|        HR|\n",
            "|  Bob| 30|        IT|\n",
            "|Cathy| 28|   Finance|\n",
            "+-----+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(\"Age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EXtxGZpt6ld",
        "outputId": "b9fdb778-8c0e-433a-c6d3-bb0a55cf882a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+\n",
            "| Name|   Dept|\n",
            "+-----+-------+\n",
            "|Alice|     HR|\n",
            "|  Bob|     IT|\n",
            "|Cathy|Finance|\n",
            "+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aggregation & Grouping"
      ],
      "metadata": {
        "id": "OX2RoKRyuOuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, sum, max, min, count\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DF_Aggregations\").getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", \"HR\", 3000),\n",
        "        (\"Bob\", \"IT\", 4000),\n",
        "        (\"Cathy\", \"HR\", 3500),\n",
        "        (\"David\", \"IT\", 4500),\n",
        "        (\"Eva\", \"Finance\", 5000)]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Dept\", \"Salary\"])\n"
      ],
      "metadata": {
        "id": "oSqwDrfsudld"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Grouping data** is the process of collecting rows that have the same values in one or more columns into a single group, which is a necessary step before performing aggregate functions on each group.\n",
        "\n",
        "The `groupBy()` function is used to create a **`GroupedData`** object, which is a temporary structure that organizes rows with the same values into groups before an aggregation is applied."
      ],
      "metadata": {
        "id": "DDT2kZ2oudRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Dept\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjL1XHMputD9",
        "outputId": "130351d8-9c01-4aa6-bbeb-2408b7351b4b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Dept|count|\n",
            "+-------+-----+\n",
            "|     HR|    2|\n",
            "|     IT|    2|\n",
            "|Finance|    1|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Single aggregations** are operations that perform a calculation on an entire DataFrame and return a single value, such as finding the `sum()`, `max()`, `min()`, or `count()` of a column.\n",
        "\n",
        "| Function | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`count()`** | Returns the number of rows in the DataFrame. | `df.count()` |\n",
        "| **`sum()`** | Calculates the sum of values in a column. | `df.agg(sum(\"sales\"))` |\n",
        "| **`avg()`** | Calculates the average value of a column. | `df.agg(avg(\"price\"))` |\n",
        "| **`min()`** | Finds the minimum value in a column. | `df.agg(min(\"age\"))` |\n",
        "| **`max()`** | Finds the maximum value in a column. | `df.agg(max(\"age\"))` |\n",
        "| **`collect_list()`** | Gathers all values from a column into a single list. | `df.agg(collect_list(\"product\"))` |\n",
        "| **`collect_set()`** | Gathers all unique values from a column into a single list. | `df.agg(collect_set(\"product\"))` |\n",
        "| **`countDistinct()`**| Returns the number of unique values in a column. | `df.agg(countDistinct(\"category\"))` |"
      ],
      "metadata": {
        "id": "yr18Erl6uUSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Dept\").avg(\"Salary\").show()   # Average salary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlUQMeiKu6TV",
        "outputId": "43f001ed-b6ff-4103-f470-6d207b7df05f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   Dept|avg(Salary)|\n",
            "+-------+-----------+\n",
            "|     HR|     3250.0|\n",
            "|     IT|     4250.0|\n",
            "|Finance|     5000.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Dept\").sum(\"Salary\").show()   # Total salary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-VIY46FvFH0",
        "outputId": "01474a6c-f3e9-4eba-9f4f-d224b64febfc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   Dept|sum(Salary)|\n",
            "+-------+-----------+\n",
            "|     HR|       6500|\n",
            "|     IT|       8500|\n",
            "|Finance|       5000|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Dept\").max(\"Salary\").show()   # Max salary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW-BY7pyvI1U",
        "outputId": "478231ff-d4f8-451b-d16b-68492c7d9259"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|   Dept|max(Salary)|\n",
            "+-------+-----------+\n",
            "|     HR|       3500|\n",
            "|     IT|       4500|\n",
            "|Finance|       5000|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Multiple aggregations** are operations that perform several different aggregate functions on the same DataFrame or grouped data, such as calculating the `sum`, `average`, and `count` all at once.\n",
        "\n",
        "* `agg()` function is used to apply one or more aggregate functions to a DataFrame, while\n",
        "*  `alias()` is used to give the resulting new column a custom, more descriptive name."
      ],
      "metadata": {
        "id": "eyvsSsyxvRpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Dept\").agg(\n",
        "    count(\"*\").alias(\"Count\"),\n",
        "    avg(\"Salary\").alias(\"AvgSalary\"),\n",
        "    sum(\"Salary\").alias(\"TotalSalary\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ty85E-JvQ1d",
        "outputId": "40a57cae-8e5b-4c9e-eb71-dc4bc29b8034"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+---------+-----------+\n",
            "|   Dept|Count|AvgSalary|TotalSalary|\n",
            "+-------+-----+---------+-----------+\n",
            "|     HR|    2|   3250.0|       6500|\n",
            "|     IT|    2|   4250.0|       8500|\n",
            "|Finance|    1|   5000.0|       5000|\n",
            "+-------+-----+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code groups the data by the **Dept** column and then performs **multiple aggregations** in a single step.\n",
        "\n",
        "* `count(\"*\")` → counts how many rows (employees) are in each department.\n",
        "* `avg(\"Salary\")` → calculates the average salary within each department.\n",
        "* `sum(\"Salary\")` → adds up all the salaries for that department.\n",
        "* `.alias(\"...\")` → renames the result columns to make the output more readable.\n"
      ],
      "metadata": {
        "id": "tBTeQ1j8v5Is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joins"
      ],
      "metadata": {
        "id": "brRfFxPawd3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"JoinCheatSheet\").getOrCreate()\n",
        "\n",
        "# Sample DataFrame 1\n",
        "employees = spark.createDataFrame([\n",
        "    (1, \"Alice\", 10),\n",
        "    (2, \"Bob\", 20),\n",
        "    (3, \"Charlie\", 30),\n",
        "    (4, \"David\", 10)\n",
        "], [\"EmpID\", \"Name\", \"DeptID\"])\n",
        "\n",
        "# Sample DataFrame 2\n",
        "departments = spark.createDataFrame([\n",
        "    (10, \"HR\"),\n",
        "    (20, \"Finance\"),\n",
        "    (40, \"IT\")\n",
        "], [\"DeptID\", \"DeptName\"])"
      ],
      "metadata": {
        "id": "m5OEmQArwe5k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Inner join** combines two DataFrames and returns a new DataFrame containing only the rows where the join key exists in **both** DataFrames."
      ],
      "metadata": {
        "id": "Ua8cchJ5xNAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.join(departments, \"DeptID\", \"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJuyXegBxj60",
        "outputId": "659de2e4-1432-4c9a-d260-5b612068fa28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+--------+\n",
            "|DeptID|EmpID| Name|DeptName|\n",
            "+------+-----+-----+--------+\n",
            "|    10|    1|Alice|      HR|\n",
            "|    10|    4|David|      HR|\n",
            "|    20|    2|  Bob| Finance|\n",
            "+------+-----+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Left join** combines two DataFrames and returns a new DataFrame containing all rows from the **left** DataFrame and the matching rows from the right DataFrame; for non-matching rows, the columns from the right DataFrame will contain `null` values."
      ],
      "metadata": {
        "id": "wsvRVivmxw0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.join(departments, \"DeptID\", \"left\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wy3zZfYjx5Gj",
        "outputId": "6010fa36-2a41-43dd-fdda-7a08946ac969"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+--------+\n",
            "|DeptID|EmpID|   Name|DeptName|\n",
            "+------+-----+-------+--------+\n",
            "|    10|    1|  Alice|      HR|\n",
            "|    20|    2|    Bob| Finance|\n",
            "|    10|    4|  David|      HR|\n",
            "|    30|    3|Charlie|    NULL|\n",
            "+------+-----+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Right join** combines two DataFrames and returns a new DataFrame containing all rows from the **right** DataFrame and the matching rows from the left DataFrame; for non-matching rows, the columns from the left DataFrame will contain `null` values."
      ],
      "metadata": {
        "id": "2LwYTBPix7wM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.join(departments, \"DeptID\", \"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFAsOqBYx_Zk",
        "outputId": "0ba29ea0-580a-447e-fcfb-6d34f8c1cbec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+--------+\n",
            "|DeptID|EmpID| Name|DeptName|\n",
            "+------+-----+-----+--------+\n",
            "|    10|    4|David|      HR|\n",
            "|    10|    1|Alice|      HR|\n",
            "|    20|    2|  Bob| Finance|\n",
            "|    40| NULL| NULL|      IT|\n",
            "+------+-----+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Full outer join** combines two DataFrames and returns a new DataFrame containing all rows from **both** DataFrames; where no match is found, the missing columns from either DataFrame are filled with `null` values."
      ],
      "metadata": {
        "id": "ahKmfGCKyF6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.join(departments, \"DeptID\", \"outer\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eYoS5ERyMql",
        "outputId": "55b73489-277b-4a44-9322-919d54f078d2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+--------+\n",
            "|DeptID|EmpID|   Name|DeptName|\n",
            "+------+-----+-------+--------+\n",
            "|    10|    1|  Alice|      HR|\n",
            "|    10|    4|  David|      HR|\n",
            "|    20|    2|    Bob| Finance|\n",
            "|    30|    3|Charlie|    NULL|\n",
            "|    40| NULL|   NULL|      IT|\n",
            "+------+-----+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Left semi join** combines two DataFrames and returns a new DataFrame containing all rows from the **left** DataFrame that have at least one match in the right DataFrame, but it only returns the columns from the left DataFrame."
      ],
      "metadata": {
        "id": "oVGuqE7zyPFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.join(departments, \"DeptID\", \"left_semi\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPs5ccZOyUZE",
        "outputId": "679f756d-e792-4194-f57c-3372f4ca00b0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|DeptID|EmpID| Name|\n",
            "+------+-----+-----+\n",
            "|    10|    1|Alice|\n",
            "|    10|    4|David|\n",
            "|    20|    2|  Bob|\n",
            "+------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Left anti join** combines two DataFrames and returns a new DataFrame containing all rows from the **left** DataFrame that **do not** have a match in the right DataFrame."
      ],
      "metadata": {
        "id": "tpqezmRByYSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employees.join(departments, \"DeptID\", \"left_anti\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ4F1YEgyYlV",
        "outputId": "0c588fd3-50ed-44ed-8b44-7653f22a16f9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+\n",
            "|DeptID|EmpID|   Name|\n",
            "+------+-----+-------+\n",
            "|    30|    3|Charlie|\n",
            "+------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sorting and Ordering"
      ],
      "metadata": {
        "id": "rVQu9PU2ydSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Function | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`orderBy()`** | Sorts the DataFrame's rows by one or more specified columns. It is a direct alias of the `sort()` function and can be used interchangeably. | `df.orderBy(\"name\", desc(\"age\"))` |\n",
        "| **`sort()`** | Sorts the DataFrame's rows by one or more specified columns. It is functionally identical to the `orderBy()` function. | `df.sort(\"salary\")` |"
      ],
      "metadata": {
        "id": "ECc_BDZ8yxwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# orderBy() - explicit, more flexible\n",
        "df.orderBy(\"Salary\").show()             # Ascending by default\n",
        "df.orderBy(df.Salary.desc()).show()     # Descending"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcJ5txoDzIHc",
        "outputId": "c4740e4d-1534-40ca-e25b-0b4947335e26"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+------+\n",
            "| Name|   Dept|Salary|\n",
            "+-----+-------+------+\n",
            "|Alice|     HR|  3000|\n",
            "|Cathy|     HR|  3500|\n",
            "|  Bob|     IT|  4000|\n",
            "|David|     IT|  4500|\n",
            "|  Eva|Finance|  5000|\n",
            "+-----+-------+------+\n",
            "\n",
            "+-----+-------+------+\n",
            "| Name|   Dept|Salary|\n",
            "+-----+-------+------+\n",
            "|  Eva|Finance|  5000|\n",
            "|David|     IT|  4500|\n",
            "|  Bob|     IT|  4000|\n",
            "|Cathy|     HR|  3500|\n",
            "|Alice|     HR|  3000|\n",
            "+-----+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort() - alias of orderBy()\n",
        "df.sort(\"Name\").show()                  # Ascending\n",
        "df.sort(df.Name.desc()).show()          # Descending"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4Tc_EQkzQGN",
        "outputId": "b71aca24-b916-4963-abf0-430ed3517f96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+------+\n",
            "| Name|   Dept|Salary|\n",
            "+-----+-------+------+\n",
            "|Alice|     HR|  3000|\n",
            "|  Bob|     IT|  4000|\n",
            "|Cathy|     HR|  3500|\n",
            "|David|     IT|  4500|\n",
            "|  Eva|Finance|  5000|\n",
            "+-----+-------+------+\n",
            "\n",
            "+-----+-------+------+\n",
            "| Name|   Dept|Salary|\n",
            "+-----+-------+------+\n",
            "|  Eva|Finance|  5000|\n",
            "|David|     IT|  4500|\n",
            "|Cathy|     HR|  3500|\n",
            "|  Bob|     IT|  4000|\n",
            "|Alice|     HR|  3000|\n",
            "+-----+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiple columns\n",
        "df.orderBy(\"Dept\", df.Salary.desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WOGIsVzzS3E",
        "outputId": "15a9443f-e86c-46be-a1a7-b273e7aefabd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+------+\n",
            "| Name|   Dept|Salary|\n",
            "+-----+-------+------+\n",
            "|  Eva|Finance|  5000|\n",
            "|Cathy|     HR|  3500|\n",
            "|Alice|     HR|  3000|\n",
            "|David|     IT|  4500|\n",
            "|  Bob|     IT|  4000|\n",
            "+-----+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data"
      ],
      "metadata": {
        "id": "Qfur13C5zw7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Function | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`dropna()`** | Removes rows from a DataFrame that contain null or `NaN` values. | `df.dropna()` |\n",
        "| **`fillna()`** | Replaces null or `NaN` values in a DataFrame with a specified value. | `df.fillna(0)` |"
      ],
      "metadata": {
        "id": "zilJHKJ1zvjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame with nulls\n",
        "data = [(\"Alice\", 30, None), (\"Bob\", None, 4000), (\"Cathy\", 29, 5000), (\"David\", None, None)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Salary\"])\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBOTwNt7z0hz",
        "outputId": "fffc3363-acc9-4f77-8009-a45012e76b61"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+\n",
            "| Name| Age|Salary|\n",
            "+-----+----+------+\n",
            "|Alice|  30|  NULL|\n",
            "|  Bob|NULL|  4000|\n",
            "|Cathy|  29|  5000|\n",
            "|David|NULL|  NULL|\n",
            "+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any null\n",
        "df.dropna().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PrmWNy3z72j",
        "outputId": "8acd5284-7ea3-401d-ad55-4a5e5de0c1ca"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+\n",
            "| Name|Age|Salary|\n",
            "+-----+---+------+\n",
            "|Cathy| 29|  5000|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where ALL values are null\n",
        "df.dropna(how=\"all\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOn7wvQ4z_ks",
        "outputId": "58f41eb5-9eda-472a-d339-3773d0e2e898"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+------+\n",
            "| Name| Age|Salary|\n",
            "+-----+----+------+\n",
            "|Alice|  30|  NULL|\n",
            "|  Bob|NULL|  4000|\n",
            "|Cathy|  29|  5000|\n",
            "|David|NULL|  NULL|\n",
            "+-----+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows if 'Age' is null\n",
        "df.dropna(subset=[\"Age\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBprIAai0CT9",
        "outputId": "71a6d80d-940f-4a2b-f69e-02eb2c81f221"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+\n",
            "| Name|Age|Salary|\n",
            "+-----+---+------+\n",
            "|Alice| 30|  NULL|\n",
            "|Cathy| 29|  5000|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill nulls with constant values\n",
        "df.fillna({\"Age\": 0, \"Salary\": 0}).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj9xcYXB0FAr",
        "outputId": "b4e30130-3d78-46b8-fad2-6c7f499f12a8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+\n",
            "| Name|Age|Salary|\n",
            "+-----+---+------+\n",
            "|Alice| 30|     0|\n",
            "|  Bob|  0|  4000|\n",
            "|Cathy| 29|  5000|\n",
            "|David|  0|     0|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill all numeric nulls with 100\n",
        "df.fillna(100).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvV6iFUB0GXL",
        "outputId": "9a06eb24-0284-4abe-aa22-208815b746fe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+\n",
            "| Name|Age|Salary|\n",
            "+-----+---+------+\n",
            "|Alice| 30|   100|\n",
            "|  Bob|100|  4000|\n",
            "|Cathy| 29|  5000|\n",
            "|David|100|   100|\n",
            "+-----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with Dates and Timestamps"
      ],
      "metadata": {
        "id": "iS_EdwAm0mpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working with dates and timestamps** involves using built-in PySpark functions to parse, format, and manipulate date and time data within a DataFrame.\n",
        "\n",
        "| Function | Description | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| **`current_date()`** | Returns the current date as a `DateType` column. | `current_date()` |\n",
        "| **`current_timestamp()`** | Returns the current date and time as a `TimestampType` column. | `current_timestamp()` |\n",
        "| **`date_add()`** | Adds a specified number of days to a given date. | `date_add(\"birth_date\", 30)` |\n",
        "| **`datediff()`** | Calculates the difference in days between two dates. | `datediff(\"end_date\", \"start_date\")` |"
      ],
      "metadata": {
        "id": "zjHQl8hk01a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date, current_timestamp, datediff, date_add\n",
        "\n",
        "# Create a simple DataFrame\n",
        "df = spark.createDataFrame([(\"Alice\",)], [\"Name\"])"
      ],
      "metadata": {
        "id": "AanrtLad0noD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Adding a Column with the Current Date**"
      ],
      "metadata": {
        "id": "ACHLg50j1Ggc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current date\n",
        "df.withColumn(\"Today\", current_date()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkYbzzbf05y8",
        "outputId": "a6ed3226-16fa-44d8-b7ec-628791e6bef8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "| Name|     Today|\n",
            "+-----+----------+\n",
            "|Alice|2025-09-22|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Adding a Column with the Current Timestamp**"
      ],
      "metadata": {
        "id": "VlIOiy4Y1LOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 10 days to current date\n",
        "df.withColumn(\"FutureDate\", date_add(current_date(), 10)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMp4klgo1UF7",
        "outputId": "69eca57b-0dae-4d7f-c8c4-258d73658f20"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "| Name|FutureDate|\n",
            "+-----+----------+\n",
            "|Alice|2025-10-02|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Difference between two dates**"
      ],
      "metadata": {
        "id": "2XdzUu__1X7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = [(\"2025-01-01\", \"2025-01-15\")]\n",
        "df2 = spark.createDataFrame(dates, [\"StartDate\", \"EndDate\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1i7t0la1gVT",
        "outputId": "7f1fac9e-a193-4017-8ba6-199a3bd79f27"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "| StartDate|   EndDate|\n",
            "+----------+----------+\n",
            "|2025-01-01|2025-01-15|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.select(datediff(\"EndDate\", \"StartDate\").alias(\"DaysDiff\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB4jSFMg1mKr",
        "outputId": "e3732fc0-a059-4e24-cb46-746260bafc0c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|DaysDiff|\n",
            "+--------+\n",
            "|      14|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Saving DataFrames"
      ],
      "metadata": {
        "id": "eq8zztoE15oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving DataFrames** is the process of writing the contents of a PySpark DataFrame to an external storage location, such as a file system, a database, or a data warehouse."
      ],
      "metadata": {
        "id": "maORRjMEPoOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 30), (\"Bob\", 40)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])"
      ],
      "metadata": {
        "id": "_6iLWMQy15Pi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **mode** specifies how Spark should handle a file or table that already exists when you are trying to save data to that location.\n",
        "\n",
        "| Mode | Description | Behavior |\n",
        "| :--- | :--- | :--- |\n",
        "| **`overwrite`** | Replaces the existing data and its schema with the new DataFrame's content. | Deletes the old file/table and writes the new one. |\n",
        "| **`append`** | Adds the rows from the DataFrame to the end of the existing data without changing the schema. | Appends new data to the existing file/table. |\n",
        "| **`ignore`** | Does nothing if the file or table already exists. The save operation is skipped entirely. | If a file/table exists, the operation is ignored. |\n",
        "| **`error`** or **`errorifexists`** | Throws an exception and fails the write operation if the file or table already exists. This is the default mode. | Fails the save operation if a file/table already exists. |"
      ],
      "metadata": {
        "id": "tTJJh_SVP-Ao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Function | Format | Description | Example |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| `write.csv()` | **CSV** | Saves the DataFrame as a comma-separated values file. | `df.write.csv(\"path/to/data.csv\")` |\n",
        "| `write.parquet()` | **Parquet** | Saves the DataFrame in the highly efficient columnar Parquet format, ideal for big data analytics. | `df.write.parquet(\"path/to/data.parquet\")` |\n",
        "| `write.json()` | **JSON** | Saves the DataFrame as a JSON file, where each row is a separate JSON object. | `df.write.json(\"path/to/data.json\")` |\n",
        "| `write.text()` | **Text** | Saves the DataFrame as a text file, with each row converted to a string. | `df.write.text(\"path/to/data.txt\")` |\n",
        "| `write.orc()` | **ORC** | Saves the DataFrame in the Optimized Row Columnar (ORC) format, another efficient columnar storage format. | `df.write.orc(\"path/to/data.orc\")` |"
      ],
      "metadata": {
        "id": "8wfSpKjkQMlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write as CSV\n",
        "df.write.mode(\"overwrite\").csv(\"output/csv_example\")"
      ],
      "metadata": {
        "id": "ntz-nAFBP_H3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write as JSON\n",
        "df.write.mode(\"append\").json(\"output/json_example\")"
      ],
      "metadata": {
        "id": "5TdXQ-zyQShe"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write as Parquet\n",
        "df.write.mode(\"ignore\").parquet(\"output/parquet_example\")"
      ],
      "metadata": {
        "id": "J8FuHxcLQdCg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error if file exists\n",
        "df.write.mode(\"errorifexists\").csv(\"output/csv_error\")"
      ],
      "metadata": {
        "id": "kPzNtM7_Qexn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caching & Persistence"
      ],
      "metadata": {
        "id": "WfNIFleVRhZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caching and persistence** are optimization techniques used in PySpark to store a DataFrame in memory or on disk after its first computation. By doing this, you avoid re-computing the same DataFrame from scratch every time it's referenced in subsequent operations, which significantly speeds up iterative algorithms and multiple queries on the same dataset. The key functions for this are **`cache()`** and **`persist()`**."
      ],
      "metadata": {
        "id": "Q5Dv4NNzSxlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [(\"Alice\", 30), (\"Bob\", 40), (\"Cathy\", 25)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])"
      ],
      "metadata": {
        "id": "jwX_pS-8S-kn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache DataFrame in memory\n",
        "df_cached = df.cache()\n",
        "df_cached.count()   # triggers caching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5cd2WwlS_DP",
        "outputId": "3e921a6a-6965-45ae-94da-4231fac32441"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist DataFrame (memory + disk)\n",
        "from pyspark import StorageLevel\n",
        "df_persisted = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "df_persisted.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7EwARLDTCMW",
        "outputId": "562575b0-2380-4ae3-fb41-569a90290138"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}