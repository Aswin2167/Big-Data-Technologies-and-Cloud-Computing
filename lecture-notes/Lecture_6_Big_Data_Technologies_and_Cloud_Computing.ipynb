{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<body>\n",
        "  <center>\n",
        "    <img align=\"center\" src=\"https://duk.ac.in/wp-content/uploads/2021/03/logo_transparent_bg_caption.png\" width=\"200px\" padding=\"10px\"   style=\" width:300px; padding: 10px;  \" />\n",
        "  <div class=\"university-details\">\n",
        "    <h2> </h2>\n",
        "    <center><h2>Kerala University of Digital Sciences, Innovation and Technology</h2></center> <h3>(Digital University Kerala, DUK)</h3>\n",
        "    <h4><i><b>School of Digital Sciences</b></i><br></h4>\n",
        "  </div>\n",
        "  <h1>  </h1>\n",
        "  </center>\n",
        "  <h5><b>Date:</b> 04/08/2025 <br>\n",
        "<b>Venue:</b> Room Number 45<br>\n",
        "<b>Topic:</b> Spark SQL<br>\n",
        "<b>Instructor:</b> Dr. Aswin VS</h5>\n",
        "</body>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "bFgLoXplbQ6Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "249ab27b"
      },
      "source": [
        "# **Spark SQL Cheat Sheet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6498e322"
      },
      "source": [
        "## **Introduction to Spark SQL**\n",
        "\n",
        "**Spark SQL** is a Spark module for structured data processing. It provides a programming interface to work with structured and semi-structured data, allowing users to query data using SQL or a DataFrame API. Spark SQL is built on top of Spark Core and leverages the Catalyst optimizer and Tungsten execution engine for high performance. It can read and write data from various sources like Parquet, JSON, ORC, CSV, and JDBC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e975291"
      },
      "source": [
        "## **Setting up SparkSession for Spark SQL**\n",
        "\n",
        "The `SparkSession` is the entry point to any Spark functionality. To use Spark SQL, you need to create a `SparkSession`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "482cf0b6",
        "outputId": "07fa1db4-20a5-4b42-a10b-e76312563768"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkSQL_CheatSheet\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Display the SparkSession object\n",
        "print(spark)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x78dc0e77bb30>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34f917c2"
      },
      "source": [
        "## **Creating DataFrames from various sources**\n",
        "\n",
        "Spark SQL can create DataFrames from various data sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40258b0b"
      },
      "source": [
        "#### From a list of tuples (similar to PySpark DataFrame creation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587b3ab5",
        "outputId": "05ca57b1-6e8c-455a-eac0-f7bc826290e7"
      },
      "source": [
        "# Create a DataFrame from a list of tuples\n",
        "data = [(\"Alice\", 1, \"New York\"), (\"Bob\", 2, \"Los Angeles\"), (\"Charlie\", 3, \"Chicago\")]\n",
        "columns = [\"Name\", \"ID\", \"City\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----------+\n",
            "|   Name| ID|       City|\n",
            "+-------+---+-----------+\n",
            "|  Alice|  1|   New York|\n",
            "|    Bob|  2|Los Angeles|\n",
            "|Charlie|  3|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e5d26f0"
      },
      "source": [
        "#### From existing RDDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc237b7f",
        "outputId": "a2cddcf7-036b-4ba3-aa06-e441874e3bd9"
      },
      "source": [
        "# Create an RDD\n",
        "rdd = spark.sparkContext.parallelize([(\"David\", 4, \"Houston\"), (\"Eva\", 5, \"Phoenix\")])\n",
        "\n",
        "# Convert RDD to DataFrame\n",
        "df_from_rdd = rdd.toDF([\"Name\", \"ID\", \"City\"])\n",
        "\n",
        "df_from_rdd.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-------+\n",
            "| Name| ID|   City|\n",
            "+-----+---+-------+\n",
            "|David|  4|Houston|\n",
            "|  Eva|  5|Phoenix|\n",
            "+-----+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bccc5008"
      },
      "source": [
        "#### From JSON files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8260627"
      },
      "source": [
        "To demonstrate loading from JSON, let's first create a sample JSON file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57482eee"
      },
      "source": [
        "# Create a sample JSON file\n",
        "json_data = \"\"\"\n",
        "{\"name\":\"Alice\", \"age\":30}\n",
        "{\"name\":\"Bob\", \"age\":35}\n",
        "{\"name\":\"Charlie\", \"age\":28}\n",
        "\"\"\"\n",
        "with open(\"sample.json\", \"w\") as f:\n",
        "    f.write(json_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f745c6a4",
        "outputId": "a7b334f6-4db8-4743-99a7-3d55a748836f"
      },
      "source": [
        "# Load data from JSON file\n",
        "df_json = spark.read.json(\"sample.json\")\n",
        "\n",
        "df_json.show()\n",
        "df_json.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|age|   name|\n",
            "+---+-------+\n",
            "| 30|  Alice|\n",
            "| 35|    Bob|\n",
            "| 28|Charlie|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "444db10d"
      },
      "source": [
        "#### From CSV files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7af58f"
      },
      "source": [
        "To demonstrate loading from CSV, let's first create a sample CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efff3c7a"
      },
      "source": [
        "# Create a sample CSV file\n",
        "csv_data = \"\"\"name,age,city\n",
        "Alice,30,New York\n",
        "Bob,35,Los Angeles\n",
        "Charlie,28,Chicago\n",
        "\"\"\"\n",
        "with open(\"sample.csv\", \"w\") as f:\n",
        "    f.write(csv_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69b2f58b",
        "outputId": "85f8eafe-a802-46ad-e5be-2ee0fbf7a6ff"
      },
      "source": [
        "# Load data from CSV file\n",
        "df_csv = spark.read.csv(\"sample.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df_csv.show()\n",
        "df_csv.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----------+\n",
            "|   name|age|       city|\n",
            "+-------+---+-----------+\n",
            "|  Alice| 30|   New York|\n",
            "|    Bob| 35|Los Angeles|\n",
            "|Charlie| 28|    Chicago|\n",
            "+-------+---+-----------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fd0c051"
      },
      "source": [
        "#### From Parquet files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1b54eb1"
      },
      "source": [
        "To demonstrate loading from Parquet, let's first save a DataFrame as Parquet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78f2bc1"
      },
      "source": [
        "# Create a sample DataFrame and save as Parquet\n",
        "data_parquet = [(\"Product A\", 100), (\"Product B\", 150), (\"Product C\", 200)]\n",
        "df_to_parquet = spark.createDataFrame(data_parquet, [\"Product\", \"Price\"])\n",
        "df_to_parquet.write.mode(\"overwrite\").parquet(\"sample.parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ad811b",
        "outputId": "55dbf51d-d9e4-42c2-fbad-540177c98ca6"
      },
      "source": [
        "# Load data from Parquet file\n",
        "df_parquet = spark.read.parquet(\"sample.parquet\")\n",
        "\n",
        "df_parquet.show()\n",
        "df_parquet.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|  Product|Price|\n",
            "+---------+-----+\n",
            "|Product A|  100|\n",
            "|Product B|  150|\n",
            "|Product C|  200|\n",
            "+---------+-----+\n",
            "\n",
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Price: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e611d046"
      },
      "source": [
        "## **Registering DataFrames as Temporary Views**\n",
        "\n",
        "To query a DataFrame using SQL, you need to register it as a temporary view. A temporary view is session-scoped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "686e37a6"
      },
      "source": [
        "# Create a sample DataFrame\n",
        "data_view = [(\"Apple\", 1.0), (\"Banana\", 0.5), (\"Cherry\", 2.5)]\n",
        "df_view = spark.createDataFrame(data_view, [\"Fruit\", \"Price\"])\n",
        "\n",
        "# Register the DataFrame as a temporary view\n",
        "df_view.createOrReplaceTempView(\"fruits\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea454172"
      },
      "source": [
        "## **Executing SQL Queries**\n",
        "\n",
        "Once a DataFrame is registered as a temporary view, you can execute SQL queries on it using `spark.sql()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6c13e41"
      },
      "source": [
        "#### Selecting all columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1984975f",
        "outputId": "0dff4785-e66a-48a7-c379-dbad6a035ac9"
      },
      "source": [
        "# Select all columns from the 'fruits' view\n",
        "spark.sql(\"SELECT * FROM fruits\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "| Fruit|Price|\n",
            "+------+-----+\n",
            "| Apple|  1.0|\n",
            "|Banana|  0.5|\n",
            "|Cherry|  2.5|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33ed2d77"
      },
      "source": [
        "#### Selecting specific columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18d226d3",
        "outputId": "c43cef0d-a522-4bfb-d16a-ba242025f0de"
      },
      "source": [
        "# Select specific columns\n",
        "spark.sql(\"SELECT Fruit FROM fruits\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "| Fruit|\n",
            "+------+\n",
            "| Apple|\n",
            "|Banana|\n",
            "|Cherry|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7915f6b3"
      },
      "source": [
        "#### Filtering data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc1b4a36",
        "outputId": "9e77128f-b6a0-4888-e1c0-3b45020fefc2"
      },
      "source": [
        "# Filter data where Price is greater than 1.0\n",
        "spark.sql(\"SELECT * FROM fruits WHERE Price > 1.0\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "| Fruit|Price|\n",
            "+------+-----+\n",
            "|Cherry|  2.5|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "262a0abd"
      },
      "source": [
        "#### Ordering data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8185eee9",
        "outputId": "8e5809b2-7f25-4fa9-f48f-0154b8693966"
      },
      "source": [
        "# Order data by Price in descending order\n",
        "spark.sql(\"SELECT * FROM fruits ORDER BY Price DESC\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "| Fruit|Price|\n",
            "+------+-----+\n",
            "|Cherry|  2.5|\n",
            "| Apple|  1.0|\n",
            "|Banana|  0.5|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96d91f87"
      },
      "source": [
        "#### Grouping and aggregating data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12a56261"
      },
      "source": [
        "Let's create a new DataFrame with a 'Category' column for aggregation examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d0e60c7"
      },
      "source": [
        "# Create a DataFrame for aggregation\n",
        "data_agg = [(\"Apple\", \"Fruit\", 1.0), (\"Banana\", \"Fruit\", 0.5), (\"Carrot\", \"Vegetable\", 0.8), (\"Broccoli\", \"Vegetable\", 1.2)]\n",
        "df_agg = spark.createDataFrame(data_agg, [\"Item\", \"Category\", \"Price\"])\n",
        "\n",
        "# Register as a temporary view\n",
        "df_agg.createOrReplaceTempView(\"produce\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5d728d",
        "outputId": "91879624-821e-4237-e418-03ea18a1c2c3"
      },
      "source": [
        "# Calculate average price per category\n",
        "spark.sql(\"SELECT Category, AVG(Price) AS AvgPrice FROM produce GROUP BY Category\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "| Category|AvgPrice|\n",
            "+---------+--------+\n",
            "|    Fruit|    0.75|\n",
            "|Vegetable|     1.0|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eb88774"
      },
      "source": [
        "## **Joins in Spark SQL**\n",
        "\n",
        "You can perform various types of joins using SQL syntax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1127a86a"
      },
      "source": [
        "Let's create two DataFrames and register them as views for join examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44c6648a"
      },
      "source": [
        "# Create DataFrames for joins\n",
        "employees_df = spark.createDataFrame([(1, \"Alice\", 10), (2, \"Bob\", 20), (3, \"Charlie\", 30)], [\"EmpID\", \"Name\", \"DeptID\"])\n",
        "departments_df = spark.createDataFrame([(10, \"HR\"), (20, \"Finance\"), (40, \"IT\")], [\"DeptID\", \"DeptName\"])\n",
        "\n",
        "# Register as temporary views\n",
        "employees_df.createOrReplaceTempView(\"employees\")\n",
        "departments_df.createOrReplaceTempView(\"departments\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0249a7b"
      },
      "source": [
        "#### Inner Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f592b191",
        "outputId": "f341a0cf-6507-440b-efc9-a3d5c2fe5479"
      },
      "source": [
        "# Inner join employees and departments\n",
        "spark.sql(\"SELECT e.Name, d.DeptName FROM employees e INNER JOIN departments d ON e.DeptID = d.DeptID\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+\n",
            "| Name|DeptName|\n",
            "+-----+--------+\n",
            "|Alice|      HR|\n",
            "|  Bob| Finance|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f3b7d3d"
      },
      "source": [
        "#### Left Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc2019f5",
        "outputId": "cea4e57c-606e-417b-98ff-b1572ae24078"
      },
      "source": [
        "# Left join employees and departments\n",
        "spark.sql(\"SELECT e.Name, d.DeptName FROM employees e LEFT JOIN departments d ON e.DeptID = d.DeptID\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|   Name|DeptName|\n",
            "+-------+--------+\n",
            "|  Alice|      HR|\n",
            "|Charlie|    NULL|\n",
            "|    Bob| Finance|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe82371a"
      },
      "source": [
        "#### Right Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa283d82",
        "outputId": "3a2d192c-8f5a-464b-ab4b-bfc9790d7113"
      },
      "source": [
        "# Right join employees and departments\n",
        "spark.sql(\"SELECT e.Name, d.DeptName FROM employees e RIGHT JOIN departments d ON e.DeptID = d.DeptID\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+\n",
            "| Name|DeptName|\n",
            "+-----+--------+\n",
            "|Alice|      HR|\n",
            "|  Bob| Finance|\n",
            "| NULL|      IT|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7f6709d"
      },
      "source": [
        "#### Full Outer Join"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecdbdf9c",
        "outputId": "30024fa3-4539-4d5f-91ad-5fe77f58de64"
      },
      "source": [
        "# Full outer join employees and departments\n",
        "spark.sql(\"SELECT e.Name, d.DeptName FROM employees e FULL OUTER JOIN departments d ON e.DeptID = d.DeptID\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|   Name|DeptName|\n",
            "+-------+--------+\n",
            "|  Alice|      HR|\n",
            "|    Bob| Finance|\n",
            "|Charlie|    NULL|\n",
            "|   NULL|      IT|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212b11ae"
      },
      "source": [
        "## **Saving Results**\n",
        "\n",
        "You can save the result of a Spark SQL query to a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd58acb4"
      },
      "source": [
        "Let's save the result of a simple query to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b7ec238"
      },
      "source": [
        "# Select data and save to CSV\n",
        "result_df = spark.sql(\"SELECT * FROM fruits WHERE Price < 2.0\")\n",
        "result_df.write.mode(\"overwrite\").csv(\"filtered_fruits_csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0551ea77"
      },
      "source": [
        "## **Stopping SparkSession**\n",
        "\n",
        "It's good practice to stop the SparkSession when you are done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1636dd1b"
      },
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a465c51"
      },
      "source": [
        "## **Programming Questions**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b62ae34d"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Load the `sample.json` file into a DataFrame and display the schema and the first 5 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36401cd7"
      },
      "source": [
        "# Write your solution here for Question 1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5b0e996"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Load the `sample.csv` file into a DataFrame, ensuring the header is used and the schema is inferred. Display the schema and the first 5 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a6c24d0"
      },
      "source": [
        "# Write your solution here for Question 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a66e883"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "Register the DataFrame loaded from `sample.csv` as a temporary view named `csv_data`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c10d9a6"
      },
      "source": [
        "# Write your solution here for Question 3"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42de6ade"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Write a Spark SQL query to select only the 'name' and 'age' columns from the `csv_data` view. Display the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91a20a4f"
      },
      "source": [
        "# Write your solution here for Question 4"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04a47756"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "Write a Spark SQL query to filter the `csv_data` view to only include rows where the 'age' is greater than 30. Display the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39535f2d"
      },
      "source": [
        "# Write your solution here for Question 5"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66948935"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "Using the `produce` temporary view created earlier, write a Spark SQL query to find the total price for each category. Display the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c732247e"
      },
      "source": [
        "# Write your solution here for Question 6"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0449628d"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "Using the `employees` and `departments` temporary views, perform a right outer join and display the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81c8c16b"
      },
      "source": [
        "# Write your solution here for Question 7"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fa084e4"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Create a new DataFrame from a list of tuples with columns \"Product\" and \"Quantity\". Register it as a temporary view named `inventory`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe6ffde0"
      },
      "source": [
        "# Write your solution here for Question 8"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b41143"
      },
      "source": [
        "### Question 9\n",
        "\n",
        "Write a Spark SQL query to select all products from the `inventory` view where the quantity is less than 50. Display the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d083db35"
      },
      "source": [
        "# Write your solution here for Question 9"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72166cb4"
      },
      "source": [
        "### Question 10\n",
        "\n",
        "Save the result of the query from Question 9 to a Parquet file named `low_inventory_parquet`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7065f30"
      },
      "source": [
        "# Write your solution here for Question 10"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}