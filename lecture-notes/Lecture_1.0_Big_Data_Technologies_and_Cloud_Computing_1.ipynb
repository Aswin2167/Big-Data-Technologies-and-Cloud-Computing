{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Definition of Data**:\n",
        "\n"
      ],
      "metadata": {
        "id": "UCq9OM2huMKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of data science, **data** refers to raw, unorganized facts, figures, or information that are collected and processed for analysis. Data can exist in various forms and structures, each requiring different methods and tools for storage, processing, and analysis. Understanding the nature of data is fundamental to effectively working with it.\n",
        "\n",
        "### Structured Data\n",
        "\n",
        "**Structured data** is highly organized and follows a fixed schema or model. It is typically stored in tabular formats with rows and columns, where each column represents a specific attribute and each row represents a record. This rigid structure makes it easy to store, query, and analyze using traditional methods.\n",
        "\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Structured_Data.png?raw=true\"  width=\"500\"/>\n",
        "\n",
        "**Key Features:**\n",
        "*   Follows a predefined schema.\n",
        "*   Organized in tables with rows and columns.\n",
        "*   Easy to search and analyze using SQL and other traditional tools.\n",
        "*   Requires less storage space compared to other types.\n",
        "\n",
        "**Examples:**\n",
        "*   **Relational Databases:** Data stored in tables with defined relationships between them (e.g., customer information, sales records).\n",
        "*   **Spreadsheets:** Data organized in rows and columns within applications like Microsoft Excel or Google Sheets.\n",
        "*   **CSV files:** Data stored in plain text format where values are separated by commas, often representing tabular data.\n",
        "\n",
        "### Semi-Structured Data\n",
        "\n",
        "**Semi-structured data** has some organizational properties but does not conform to a strict, fixed schema like structured data. It contains tags or markers to separate and identify data elements, allowing for hierarchical relationships. While it doesn't fit into a relational database, its organizational features make it more manageable than unstructured data.\n",
        "\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Semi-Structured_Data.png?raw=true\"  width=\"500\"/>\n",
        "\n",
        "**Key Features:**\n",
        "*   Contains tags or markers to delineate elements.\n",
        "*   Does not follow a rigid schema.\n",
        "*   Supports hierarchical structures.\n",
        "*   More flexible than structured data.\n",
        "\n",
        "**Examples:**\n",
        "*   **JSON (JavaScript Object Notation):** A lightweight data interchange format that uses key-value pairs and arrays to represent data. Widely used in web applications and APIs.\n",
        "*   **XML (eXtensible Markup Language):** A markup language that uses tags to define elements and their attributes, commonly used for data exchange and document structuring.\n",
        "*   **NoSQL Databases (some types):** Databases like document databases (e.g., MongoDB) often store data in semi-structured formats like JSON.\n",
        "\n",
        "### Unstructured Data\n",
        "\n",
        "**Unstructured data** lacks a predefined schema or organization. It exists in its raw form and does not fit neatly into traditional rows and columns. Analyzing unstructured data often requires more advanced techniques like natural language processing (NLP), machine learning, and data mining.\n",
        "\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/Unstructured_Data.png?raw=true\"  width=\"500\"/>\n",
        "\n",
        "**Key Features:**\n",
        "*   No predefined schema or structure.\n",
        "*   Difficult to search and analyze using traditional methods.\n",
        "*   Requires more sophisticated tools and techniques for processing.\n",
        "*   Represents the vast majority of data generated today.\n",
        "\n",
        "**Examples:**\n",
        "*   **Text Documents:** Emails, articles, social media posts, books, reports.\n",
        "*   **Images:** Photographs, scans, graphics.\n",
        "*   **Audio Files:** Recordings, music, podcasts.\n",
        "*   **Video Files:** Movies, surveillance footage, video clips.\n",
        "*   **Sensor Data:** Raw data streams from sensors before processing and structuring.\n",
        "\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/data_formats_example.png?raw=true\"  width=\"1000\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "hKk61C7puGCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Types**"
      ],
      "metadata": {
        "id": "QUBglHIjdtYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding different **data types** is crucial in data analysis and machine learning. The type of data dictates the appropriate methods for storage, cleaning, analysis, and visualization. Data can be broadly categorized into several types based on its nature and properties.\n",
        "\n",
        "  <img align=\"right\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/data_types_and_examples.png?raw=true\"  width=\"600\"/>\n",
        "  \n",
        "### Numerical Data\n",
        "\n",
        "**Numerical data** represents quantitative values, meaning they can be measured and expressed as numbers. Numerical data can be further divided into two sub-types:\n",
        "\n",
        "*   **Discrete Numerical Data:** Represents countable items or values that can only take specific, distinct values. There are a finite or countable number of possible values.\n",
        "    *   **Examples:**\n",
        "        *   Number of students in a class (you can't have half a student).\n",
        "        *   Number of cars passing a point in an hour.\n",
        "        *   The score on a die roll (1, 2, 3, 4, 5, or 6).\n",
        "        *   Number of defects in a manufactured product.\n",
        "\n",
        "*   **Continuous Numerical Data:** Represents measurements that can take any value within a given range. The values are not restricted to specific steps or integers.\n",
        "    *   **Examples:**\n",
        "        *   Height of a person (can be 1.75 meters, 1.755 meters, etc.).\n",
        "        *   Temperature of a room (can be 22.1°C, 22.15°C, etc.).\n",
        "        *   Weight of an object.\n",
        "        *   Time taken to complete a task.\n",
        "\n",
        "### Categorical Data\n",
        "\n",
        "**Categorical data** represents qualitative values or labels. These values belong to a finite set of categories or groups. Categorical data can be further classified into two sub-types:\n",
        "\n",
        "*   **Nominal Categorical Data:** Represents categories that do not have a natural order or ranking. The categories are simply names or labels.\n",
        "    *   **Examples:**\n",
        "        *   Colors (Red, Blue, Green).\n",
        "        *   Marital Status (Single, Married, Divorced).\n",
        "        *   Blood Type (A, B, AB, O).\n",
        "        *   Gender (Male, Female, Non-binary).\n",
        "\n",
        "*   **Ordinal Categorical Data:** Represents categories that have a natural order or ranking. The difference between categories is not necessarily uniform or measurable, but the order is meaningful.\n",
        "    *   **Examples:**\n",
        "        *   Educational Level (High School, Bachelor's, Master's, PhD).\n",
        "        *   Customer Satisfaction Rating (Very Dissatisfied, Dissatisfied, Neutral, Satisfied, Very Satisfied).\n",
        "        *   Size of Clothing (Small, Medium, Large).\n",
        "        *   Military Ranks.\n",
        "\n",
        "### Time-Series Data\n",
        "\n",
        "**Time-series data** is a sequence of data points collected over a period of time, typically at equally spaced intervals. The order of the data points is crucial, as it often reveals trends, seasonality, and other temporal patterns.\n",
        "\n",
        "*   **Key Characteristics:**\n",
        "    *   Ordered by time.\n",
        "    *   Data points collected at specific time intervals.\n",
        "    *   Often exhibits trends, seasonality, and cyclical patterns.\n",
        "\n",
        "*   **Examples:**\n",
        "    *   Stock prices recorded daily.\n",
        "    *   Hourly temperature readings.\n",
        "    *   Monthly sales figures.\n",
        "    *   Website traffic recorded minute by minute.\n",
        "\n",
        "### Spatial Data\n",
        "\n",
        "**Spatial data**, also known as geospatial data, represents information that has a geographical or locational component. It describes the position, shape, and relationships of features on the Earth's surface or in space.\n",
        "\n",
        "*   **Relevance:** Used in Geographic Information Systems (GIS), mapping, navigation, urban planning, environmental studies, and more.\n",
        "\n",
        "*   **Examples:**\n",
        "    *   Locations of cities on a map (points).\n",
        "    *   Boundaries of countries or states (polygons).\n",
        "    *   Road networks (lines).\n",
        "    *   Satellite imagery.\n",
        "    *   GPS coordinates.\n",
        "\n",
        "### Multimedia Data\n",
        "\n",
        "**Multimedia data** encompasses various forms of data, including text, images, audio, and video. It is often unstructured or semi-structured and requires specialized techniques for processing and analysis.\n",
        "\n",
        "*   **Various Forms:**\n",
        "    *   **Images:** Photos, illustrations, scans.\n",
        "    *   **Audio:** Music, speech, sound effects.\n",
        "    *   **Video:** Movies, recordings, animations.\n",
        "    *   **Text:** Documents, captions, transcripts (when combined with other media).\n",
        "\n",
        "*   **Examples:**\n",
        "    *   A video file containing both visual and audio information.\n",
        "    *   An image with embedded text captions.\n",
        "    *   A presentation with text, images, and audio clips.\n",
        "    *   Social media posts containing text, images, and videos.\n",
        "\n",
        "Understanding these different data types is fundamental for selecting appropriate data collection methods, choosing the right analytical tools and techniques, and effectively interpreting the results of data analysis."
      ],
      "metadata": {
        "id": "k8ZD5CozdpQt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a8f7435"
      },
      "source": [
        "## Traditional Dimensions of Big Data (3 Vs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The concept of \"Big Data\" is often characterized by several key dimensions that highlight its complexity and the challenges it presents compared to traditional data. The most commonly cited dimensions are the \"3 Vs\": Volume, Velocity, and Variety. These dimensions were first introduced by Gartner analyst Doug Laney in 2001.\n",
        "\n",
        "### Volume\n",
        "\n",
        "**Volume** refers to the sheer scale of data being generated, stored, and processed. Big Data involves datasets that are so large they cannot be managed or analyzed effectively using traditional database and data processing tools.\n",
        "\n",
        "*   **Explanation:** As data sources proliferate (social media, sensors, transactions, etc.), the amount of data grows exponentially, reaching petabytes, exabytes, or even zettabytes. This massive scale requires distributed storage systems and parallel processing frameworks.\n",
        "*   **Importance/Challenge:** Handling such vast quantities of data poses significant challenges in terms of storage costs, processing power, and the time required for analysis. It necessitates scalable infrastructure and efficient data management techniques.\n",
        "*   **Example:** A social media platform storing billions of user posts, comments, and interactions daily. A large hadron collider generating petabytes of experimental data every second.\n",
        "\n",
        "### Velocity\n",
        "\n",
        "**Velocity** refers to the speed at which data is generated, collected, and processed. In the context of Big Data, data is often streaming in real-time or near real-time, requiring rapid processing and analysis to extract timely insights.\n",
        "\n",
        "*   **Explanation:** Data flows in at unprecedented speeds, often from sources like sensors, financial markets, and online interactions. The value of this data often diminishes quickly, making immediate processing essential for timely decision-making.\n",
        "*   **Importance/Challenge:** Dealing with high-velocity data requires real-time data processing capabilities, stream analytics, and low-latency systems. Traditional batch processing methods are often inadequate.\n",
        "*   **Example:** Real-time stock trading data, where decisions need to be made in milliseconds. Sensor data from autonomous vehicles being processed instantly to navigate and avoid obstacles.\n",
        "\n",
        "### Variety\n",
        "\n",
        "**Variety** refers to the different forms and sources of data. Big Data encompasses structured data (like databases), semi-structured data (like JSON or XML), and unstructured data (like text, images, audio, and video).\n",
        "\n",
        "*   **Explanation:** Data no longer fits neatly into traditional, structured formats. It comes from diverse sources and in disparate formats, making it challenging to integrate, clean, and analyze using traditional methods designed for structured data.\n",
        "*   **Importance/Challenge:** Integrating and analyzing data from such a wide range of formats and sources requires flexible data models, advanced data integration techniques, and analytical tools capable of handling different data types.\n",
        "*   **Example:** A company analyzing customer feedback from structured sales databases, semi-structured web server logs, and unstructured social media posts and customer service call transcripts.\n",
        "\n",
        "  <img align=\"centre\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/3Vs.png?raw=true\"  width=\"600\"/>"
      ],
      "metadata": {
        "id": "DcNDZLqeWQjV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96a1a187"
      },
      "source": [
        "## **Extended dimensions (up to 7–10 vs)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2734788d"
      },
      "source": [
        "Beyond the traditional \"3 Vs\" (Volume, Velocity, Variety), several other dimensions have been proposed to provide a more comprehensive understanding of the complexities and challenges associated with Big Data. These extended 'Vs' highlight aspects related to the quality, trustworthiness, value, and usability of the data.\n",
        "\n",
        "### Veracity\n",
        "\n",
        "**Veracity** refers to the truthfulness, accuracy, and trustworthiness of the data. Big Data often comes from diverse sources, some of which may not be reliable, leading to uncertainty and inaccuracies.\n",
        "\n",
        "*   **Explanation:** Data can be noisy, inconsistent, biased, or subject to manipulation. Ensuring the quality and reliability of data is critical for deriving meaningful and actionable insights. Low veracity can lead to flawed analysis and poor decision-making.\n",
        "*   **Significance/Challenge:** Dealing with data uncertainty and ensuring data quality requires robust data cleaning, validation, and governance processes. It's essential to understand the source and potential biases of the data.\n",
        "*   **Example:** Analyzing social media sentiment where posts might contain sarcasm, slang, or fake information, making it difficult to accurately gauge public opinion. Sensor data from faulty equipment providing inaccurate readings.\n",
        "\n",
        "### Value\n",
        "\n",
        "**Value** refers to the potential to transform Big Data into meaningful and useful insights that lead to business value, competitive advantage, or societal benefit. Simply having large amounts of data is not enough; the data must be processed and analyzed to extract value.\n",
        "\n",
        "*   **Explanation:** The ultimate goal of collecting and analyzing Big Data is to generate value. This involves identifying relevant data, applying appropriate analytical techniques, and translating the findings into actionable strategies or outcomes.\n",
        "*   **Significance/Challenge:** Extracting value from Big Data requires clear objectives, skilled data scientists, appropriate tools, and the ability to integrate insights into decision-making processes. Not all data is equally valuable, and identifying the truly valuable data is a key challenge.\n",
        "*   **Example:** A retail company analyzing customer purchase history, website clicks, and loyalty program data to personalize product recommendations and increase sales. A healthcare provider using patient data to predict disease outbreaks and allocate resources effectively.\n",
        "\n",
        "### Variability\n",
        "\n",
        "**Variability** refers to the inconsistency in the data flow rate and the changes in the structure of the data over time. This is distinct from Variety, which refers to the different types of data formats. Variability deals with the dynamic nature of the data itself.\n",
        "\n",
        "*   **Explanation:** The speed and format of incoming data streams can fluctuate significantly. For instance, data from a popular event might spike, while data from a less active source might trickle in slowly. The meaning or context of data points can also change.\n",
        "*   **Significance/Challenge:** Managing variability requires flexible processing systems that can adapt to changing data rates and structures. It poses challenges for maintaining consistent data pipelines and ensuring real-time processing capabilities.\n",
        "*   **Example:** Website traffic data showing huge spikes during a major online sale event versus normal traffic levels. Sensor data from a manufacturing plant where data streams might vary in frequency depending on the production cycle.\n",
        "\n",
        "### Visualization\n",
        "\n",
        "**Visualization** refers to the ability to effectively present Big Data and the insights derived from it in a clear, understandable, and interactive graphical format. Given the complexity and scale of Big Data, traditional visualization methods are often insufficient.\n",
        "\n",
        "*   **Explanation:** Visualizing Big Data helps in exploring patterns, identifying trends, and communicating findings to stakeholders who may not have a technical background. Effective visualization tools are needed to handle the volume and complexity of the data.\n",
        "*   **Significance/Challenge:** Creating meaningful and scalable visualizations for massive datasets is challenging. It requires specialized tools and techniques to avoid clutter and effectively convey information without overwhelming the viewer.\n",
        "*   **Example:** Using interactive dashboards to display real-time performance metrics from various business units. Creating network graphs to visualize connections in social media data.\n",
        "\n",
        "### Validity\n",
        "\n",
        "**Validity** refers to the correctness and accuracy of the data for its intended use. While Veracity is about the truthfulness of the data itself, Validity is about whether the data is appropriate and relevant for a specific analytical task.\n",
        "\n",
        "*   **Explanation:** Data might be accurate in its raw form (high veracity) but might not be valid for a particular analysis if it's outdated, incomplete for the required scope, or collected under different conditions than the analysis assumes.\n",
        "*   **Significance/Challenge:** Ensuring data validity requires a deep understanding of the analytical problem, the data sources, and potential limitations of the data. It involves assessing the data's relevance and suitability for the specific question being asked.\n",
        "*   **Example:** Using historical sales data from a period before a major product redesign to predict current sales trends – the data might be accurate (high veracity) but not valid for predicting current trends due to the change.\n",
        "\n",
        "### Vulnerability\n",
        "\n",
        "**Vulnerability** refers to the security risks associated with storing, processing, and analyzing large and diverse datasets, especially those containing sensitive information. The sheer volume and variety of Big Data increase the potential attack surface.\n",
        "\n",
        "*   **Explanation:** Big Data systems can be targets for cyberattacks, data breaches, and privacy violations. Protecting this data is paramount, especially in sectors like healthcare, finance, and government.\n",
        "*   **Significance/Challenge:** Securing Big Data requires robust security measures, including encryption, access control, anomaly detection, and compliance with data privacy regulations (like GDPR, CCPA). The distributed nature of many Big Data systems adds complexity to security management.\n",
        "*   **Example:** A healthcare organization storing vast amounts of patient health records needing to implement stringent security protocols to prevent breaches and comply with HIPAA regulations.\n",
        "\n",
        "These extended dimensions, along with the traditional 3 Vs, provide a more complete picture of the characteristics and challenges of working with Big Data, highlighting the need for sophisticated approaches to data management, processing, analysis, and governance.\n",
        "\n",
        "  <img align=\"centre\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/7Vs.png?raw=true\"  width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38578566"
      },
      "source": [
        "## Data lifecycle\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f44d025a"
      },
      "source": [
        "The **data lifecycle** describes the sequence of stages that data goes through from its initial creation or collection to its eventual deletion or archival. Managing data effectively at each stage is crucial for ensuring data quality, accessibility, security, and compliance, as well as for maximizing the value derived from the data.\n",
        "\n",
        "Here are the typical stages of the data lifecycle:\n",
        "\n",
        "  <img align=\"center\" src=\"https://github.com/Aswin2167/Big-Data-Technologies-and-Cloud-Computing/blob/main/lecture-notes/images/data_lifecycle.png?raw=true\"  width=\"600\"/>\n",
        "\n",
        "### 1. Data Generation\n",
        "\n",
        "This is the stage where data is created or first collected. Data can be generated from a vast array of sources, both internal and external.\n",
        "\n",
        "*   **What happens:** Data is born. This can involve recording transactions, capturing sensor readings, receiving form submissions, logging user activity, digitizing documents, or collecting data from external APIs and sources.\n",
        "*   **Purpose/Key Activities:** To create the initial raw data that will be used in subsequent stages. Activities include data entry, data capture, data acquisition, and data scraping.\n",
        "\n",
        "### 2. Data Storage\n",
        "\n",
        "Once data is generated, it needs to be stored in a secure and accessible manner. The choice of storage depends on the data type, volume, velocity, and access requirements.\n",
        "\n",
        "*   **What happens:** Raw or initially processed data is saved to various storage mediums. This can range from simple file systems and databases to data warehouses, data lakes, or cloud storage solutions.\n",
        "*   **Purpose/Key Activities:** To preserve the data for future use and processing. Activities include data ingestion, backup, security implementation, and defining storage schemas or structures.\n",
        "\n",
        "### 3. Data Processing\n",
        "\n",
        "In this stage, the raw data is transformed, cleaned, and organized into a format suitable for analysis. This often involves various data manipulation and transformation techniques.\n",
        "\n",
        "*   **What happens:** Data is cleaned (handling missing values, correcting errors, removing duplicates), transformed (aggregating, normalizing, restructuring), and integrated from multiple sources. This stage prepares the data for meaningful analysis.\n",
        "*   **Purpose/Key Activities:** To refine and prepare the data for analysis. Activities include data cleaning, transformation, integration, validation, and enrichment.\n",
        "\n",
        "### 4. Data Analysis\n",
        "\n",
        "This is where insights are extracted from the processed data. Various analytical techniques are applied to identify patterns, trends, correlations, and anomalies.\n",
        "\n",
        "*   **What happens:** Data scientists and analysts use statistical methods, machine learning algorithms, data mining techniques, and other analytical tools to explore the data and answer specific questions or test hypotheses.\n",
        "*   **Purpose/Key Activities:** To discover insights, support decision-making, build models, and derive knowledge from the data. Activities include exploratory data analysis, statistical modeling, predictive modeling, and pattern recognition.\n",
        "\n",
        "### 5. Data Visualization\n",
        "\n",
        "Once insights are gained from the analysis, data visualization is used to present these findings in a clear, understandable, and compelling graphical format.\n",
        "\n",
        "*   **What happens:** Analytical results are translated into charts, graphs, dashboards, maps, and other visual representations to communicate findings to stakeholders.\n",
        "*   **Purpose/Key Activities:** To effectively communicate complex data insights, make data more accessible and understandable, and facilitate data-driven decision-making. Activities include creating reports, dashboards, and interactive visualizations.\n",
        "\n",
        "### 6. Archiving/Deletion\n",
        "\n",
        "The final stage involves managing data that is no longer actively used. Depending on regulations, policies, and ongoing needs, data may be archived for future reference or legally compliant deletion.\n",
        "\n",
        "*   **What happens:** Data that is no longer needed for daily operations or active analysis but must be retained for compliance or historical purposes is moved to long-term, less accessible storage (archiving). Data that is no longer required and has no legal or business reason to be kept is permanently removed (deletion).\n",
        "*   **Purpose/Key Activities:** To manage data storage costs, comply with data retention policies and regulations, and protect privacy by securely disposing of sensitive data when no longer needed. Activities include data retention policy implementation, data backup for archives, and secure data erasure.\n",
        "\n",
        "These stages form a continuous cycle, where insights from analysis might lead to the generation of new data or modifications in how data is collected and processed. Effective management throughout this lifecycle is key to leveraging data as a valuable asset.\n",
        "\n"
      ]
    }
  ]
}